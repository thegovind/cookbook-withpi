{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-masthead"
      },
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://play.withpi.ai/logo/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# Supervised Fine-tuning (SFT) with Standard Gradient Descent\n",
        "\n",
        "This is the companion to the SFT playground\n",
        "\n",
        "Description: Train models to more deeply learn patterns from your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-setup-markdown"
      },
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "from google.colab import files, userdata\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "%pip install withpi litellm httpx datasets jinja2 tqdm\n",
        "\n",
        "# Import a bunch of useful libraries for later.\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from collections import defaultdict\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import datasets\n",
        "import httpx\n",
        "import litellm\n",
        "import jinja2\n",
        "from tqdm.notebook import tqdm\n",
        "from withpi import PiClient\n",
        "from withpi.types import Contract\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "\n",
        "def print_contract(contract: Contract):\n",
        "    \"\"\"print_contract pretty-prints a contract\"\"\"\n",
        "    for dimension in contract.dimensions:\n",
        "        print(dimension.label)\n",
        "        for sub_dimension in dimension.sub_dimensions:\n",
        "            print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "    \"\"\"generate passes the provided system and user prompts into the given model\n",
        "    via LiteLLM\"\"\"\n",
        "    messages = [\n",
        "        {\"content\": system, \"role\": \"system\"},\n",
        "        {\"content\": user, \"role\": \"user\"},\n",
        "    ]\n",
        "    return litellm.completion(model=model, messages=messages).choices[0].message.content\n",
        "\n",
        "\n",
        "class printer(str):\n",
        "    \"\"\"printer makes strings with embedded newlines print more nicely\"\"\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self\n",
        "\n",
        "\n",
        "def print_response(response: str):\n",
        "    \"\"\"print_response pretty-prints an LLM response, respecting newlines\"\"\"\n",
        "    display(printer(response))\n",
        "\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "    \"\"\"print_scores pretty-prints a Pi Score response as a table.\"\"\"\n",
        "    for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "        print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "        for (\n",
        "            subdimension_name,\n",
        "            subdimension_score,\n",
        "        ) in dimension_scores.subdimension_scores.items():\n",
        "            print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "        print(\"\\n\")\n",
        "    print(\"---------------------\")\n",
        "    print(f\"Total score: {pi_scores.total_score}\")\n",
        "\n",
        "\n",
        "def save_file(filename: str, model: str):\n",
        "    \"\"\"save_file offers to download the model with the given filename\"\"\"\n",
        "    Path(filename).write_text(model)\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "def load_contract(url: str) -> Contract:\n",
        "    \"\"\"load_contract pulls a Contract JSON blob locally with validation.\"\"\"\n",
        "    resp = httpx.get(url)\n",
        "    return Contract.model_validate_json(resp.content)\n",
        "\n",
        "\n",
        "def load_and_split_dataset(url: str) -> datasets.DatasetDict:\n",
        "    \"\"\"load_and_split_dataset pulls in the Parquet file at url and does a 90/10 split\"\"\"\n",
        "    return datasets.load_dataset(\n",
        "        \"parquet\", data_files=url, split=\"train\"\n",
        "    ).train_test_split(test_size=0.1)\n",
        "\n",
        "\n",
        "def do_bulk_inference(dataset, system, model):\n",
        "    \"\"\"do_bulk_inference performs inference on the 'input' column of dataset, using\n",
        "    the provided system prompt.  The model identified will be used via LiteLLM\"\"\"\n",
        "\n",
        "    def do_generate(user, pbar):\n",
        "        result = generate(system, user, model)\n",
        "        pbar.update(1)\n",
        "        return result\n",
        "\n",
        "    futures = []\n",
        "    pbar = tqdm(total=len(dataset))\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for row in dataset:\n",
        "            futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "    return [future.result() for future in futures]\n",
        "\n",
        "\n",
        "def do_bulk_templated_inference(dataset, optimized, model):\n",
        "    \"\"\"do_bulk_templated_inference performs inference on the 'input' column of dataset,\n",
        "    using the provided optimized prompt.  It should be a Jinja2 template as returned\n",
        "    by DSPy\"\"\"\n",
        "    prompt_template = jinja2.Template(optimized)\n",
        "    result_extractor = re.compile(\n",
        "        r\".*\\[\\[ ## response ## \\]\\](.*)\\[\\[ ## completed ## \\]\\]\", re.DOTALL\n",
        "    )\n",
        "\n",
        "    def do_generate(prompt: str, pbar) -> str:\n",
        "        messages = json.loads(prompt_template.render(input=prompt))\n",
        "        result = (\n",
        "            litellm.completion(model=model, messages=messages)\n",
        "            .choices[0]\n",
        "            .message.content\n",
        "        )\n",
        "\n",
        "        pbar.update(1)\n",
        "        return result_extractor.match(result).group(1)\n",
        "\n",
        "    futures = []\n",
        "    pbar = tqdm(total=len(dataset))\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for row in dataset:\n",
        "            futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "    return [future.result() for future in futures]\n",
        "\n",
        "\n",
        "def generate_table(\n",
        "    job_id: str, training_data: dict, is_done: bool, additional_columns: dict[str, str]\n",
        "):\n",
        "    \"\"\"Generate a training progress table dynamically.\"\"\"\n",
        "    data_dict = {}\n",
        "    for header in [\"Step\", \"Epoch\", \"Learning_Rate\", \"Training_Loss\", \"Eval_Loss\"]:\n",
        "        data_dict[header] = []\n",
        "    for header in additional_columns.keys():\n",
        "        data_dict[header] = []\n",
        "\n",
        "    for step, data in training_data.items():\n",
        "        data_dict[\"Step\"].append(step)\n",
        "        for header, key in [\n",
        "            (\"Epoch\", \"epoch\"),\n",
        "            (\"Learning_Rate\", \"learning_rate\"),\n",
        "            (\"Training_Loss\", \"loss\"),\n",
        "            (\"Eval_Loss\", \"eval_loss\"),\n",
        "        ]:\n",
        "            data_dict[header].append(data.get(key, \"X\"))\n",
        "        for header, key in additional_columns.items():\n",
        "            data_dict[header].append(data.get(key, \"X\"))\n",
        "\n",
        "    if not is_done:\n",
        "        data_dict[\"Step\"].append(\"...\")\n",
        "        for header in [\"Epoch\", \"Learning_Rate\", \"Training_Loss\", \"Eval_Loss\"]:\n",
        "            data_dict[header].append(\"\")\n",
        "        for header in additional_columns.keys():\n",
        "            data_dict[header].append(\"\")\n",
        "\n",
        "    return pd.DataFrame(data_dict)\n",
        "\n",
        "\n",
        "def stream_response(job_id: str, method, additional_columns: dict[str, str]):\n",
        "    \"\"\"stream_response streams messages from the provided method\n",
        "\n",
        "    method should be a Pi client object with `retrieve` and `stream_messages`\n",
        "    endpoints.  This is primarily for convenience.\"\"\"\n",
        "\n",
        "    print(f\"Training Status for {job_id}\")\n",
        "\n",
        "    training_data = defaultdict(dict)\n",
        "    is_log_console = False\n",
        "\n",
        "    stream_output = display(\n",
        "        generate_table(\n",
        "            job_id, training_data, is_done=False, additional_columns=additional_columns\n",
        "        ),\n",
        "        display_id=True,\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        response = method.retrieve(job_id=job_id)\n",
        "        if (response.state != \"QUEUED\") and (response.state != \"RUNNING\"):\n",
        "            if response.state == \"DONE\" and not is_log_console:\n",
        "                for line in response.detailed_status:\n",
        "                    try:\n",
        "                        data_dict = json.loads(line)\n",
        "                        training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                stream_output.update(\n",
        "                    generate_table(\n",
        "                        job_id,\n",
        "                        training_data,\n",
        "                        is_done=True,\n",
        "                        additional_columns=additional_columns,\n",
        "                    )\n",
        "                )\n",
        "            return response\n",
        "\n",
        "        with method.with_streaming_response.stream_messages(\n",
        "            job_id=job_id, timeout=None\n",
        "        ) as response:\n",
        "            is_done = False\n",
        "            for line in response.iter_lines():\n",
        "                if line == \"DONE\":\n",
        "                    is_done = True\n",
        "                try:\n",
        "                    data_dict = json.loads(line)\n",
        "                    training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                stream_output.update(\n",
        "                    generate_table(\n",
        "                        job_id,\n",
        "                        training_data,\n",
        "                        is_done,\n",
        "                        additional_columns=additional_columns,\n",
        "                    )\n",
        "                )\n",
        "                is_log_console = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load a contract and dataset\n",
        "\n",
        "We have a pre-existing contract you can play with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "tldr_contract = load_contract(\n",
        "    \"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/tldr.json\"\n",
        ")\n",
        "\n",
        "num_examples = 200\n",
        "tldr_data = datasets.load_dataset(\"withpi/tldr\")[\"train\"].select(range(num_examples))\n",
        "\n",
        "print(tldr_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Kick off the job\n",
        "\n",
        "The SFT job internally performs a 90/10 train-test split, which is why the loader is not splitting the input data.\n",
        "\n",
        "This process takes a while, please be patient as a cloud GPU is aquired, fine tuning is performed, and a result is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvhTP20ijfNO"
      },
      "outputs": [],
      "source": [
        "status = client.model.sft.start_job(\n",
        "    contract=tldr_contract,\n",
        "    examples=[\n",
        "        {\"llm_input\": row[\"prompt\"], \"llm_output\": row[\"completion\"]}\n",
        "        for row in tldr_data\n",
        "    ],\n",
        "    base_sft_model=\"LLAMA_3.2_3B\",\n",
        "    num_train_epochs=5,\n",
        ")\n",
        "print(status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIRsdDJTyLIi"
      },
      "source": [
        "## Monitor for completion\n",
        "\n",
        "Now run the following cell to see how training is progressing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhJrCijJ8I2n"
      },
      "outputs": [],
      "source": [
        "response = stream_response(\n",
        "    status.job_id,\n",
        "    client.model.sft,\n",
        "    additional_columns={\"Pi_Score\": \"contract_score\"},\n",
        ")\n",
        "print(\"SFT model = {}\".format(response.trained_models[0].model_dump_json(indent=2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lrHY65Tyx_J"
      },
      "source": [
        "# Now Load the model!\n",
        "\n",
        "Run the following cell to put it into an inference backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhAC45LKy7Kc"
      },
      "outputs": [],
      "source": [
        "client.model.sft.load(\n",
        "    status.job_id,\n",
        ")\n",
        "\n",
        "for idx in range(200):\n",
        "  is_done = client.model.sft.retrieve(\n",
        "      status.job_id,\n",
        "  )\n",
        "  if is_done:\n",
        "    print(\"Loaded!\")\n",
        "    break\n",
        "  else:\n",
        "    time.sleep(3)\n",
        "if not is_done:\n",
        "  print(\"Did not load in time.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxRgW0EszUI1"
      },
      "source": [
        "# Query the model!\n",
        "\n",
        "Models are hosted on [Fireworks](https://fireworks.ai) accessible with your API key through your favorite frontend.  Try the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeXbW-030Prv"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"SUBREDDIT: r/relationships TITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting POST: Not sure if this belongs here but it's worth a try. Backstory: When I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. Now: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. His friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. So I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions. TL;DR:\"\"\"\n",
        "\n",
        "response = litellm.text_completion(\n",
        "    prompt=prompt,\n",
        "    model=\"fireworks_ai/unused\",\n",
        "    api_base=f\"https://api.withpi.ai/v1/model/sft/{status.job_id}\",\n",
        "    api_key=os.environ[\"WITHPI_API_KEY\"],\n",
        "    max_tokens=2048,\n",
        ")\n",
        "\n",
        "print(\"Raw Completion response:\\n\")\n",
        "print_response(response.choices[0].text)\n",
        "\n",
        "response = litellm.completion(\n",
        "    messages=[\n",
        "        {\"content\": prompt,\n",
        "         \"role\": \"user\",\n",
        "    }],\n",
        "    model=\"fireworks_ai/unused\",\n",
        "    api_base=f\"https://api.withpi.ai/v1/model/sft/{status.job_id}\",\n",
        "    api_key=os.environ[\"WITHPI_API_KEY\"],\n",
        "    max_tokens=2048\n",
        ")\n",
        "print(\"\\nChat completion:\\n\")\n",
        "print_response(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}