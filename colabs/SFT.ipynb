{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Low_Rank_Adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-masthead"
      },
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# Supervised Fine-tuning (SFT) with Standard Gradient Descent\n",
        "\n",
        "This is the companion to the SFT playground\n",
        "\n",
        "Description: Train models to more deeply learn patterns from your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-setup-markdown"
      },
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "This notebook needs a T4 GPU.  Make sure to select this explicitly above before proceeding.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install withpi httpx datasets tqdm\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import datasets\n",
        "import httpx\n",
        "from google.colab import files, userdata\n",
        "from withpi import PiClient\n",
        "from withpi.types import Contract\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.live import Live\n",
        "\n",
        "console = Console()\n",
        "\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get(\"WITHPI_API_KEY\")\n",
        "client = PiClient()\n",
        "\n",
        "\n",
        "def print_contract(contract: Contract):\n",
        "    \"\"\"print_contract pretty-prints a contract\"\"\"\n",
        "    for dimension in contract.dimensions:\n",
        "        print(dimension.label)\n",
        "        for sub_dimension in dimension.sub_dimensions:\n",
        "            print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "    \"\"\"print_scores pretty-prints a Pi Score response as a table.\"\"\"\n",
        "    for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "        print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "        for (\n",
        "            subdimension_name,\n",
        "            subdimension_score,\n",
        "        ) in dimension_scores.subdimension_scores.items():\n",
        "            print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "        print(\"\\n\")\n",
        "    print(\"---------------------\")\n",
        "    print(f\"Total score: {pi_scores.total_score}\")\n",
        "\n",
        "\n",
        "def save_file(filename: str, model: str):\n",
        "    \"\"\"save_file offers to download the model with the given filename\"\"\"\n",
        "    Path(filename).write_text(model)\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "def load_contract(url: str) -> Contract:\n",
        "    \"\"\"load_contract pulls a Contract JSON blob locally with validation.\"\"\"\n",
        "    resp = httpx.get(url)\n",
        "    return Contract.model_validate_json(resp.content)\n",
        "\n",
        "\n",
        "def generate_table(training_data: dict, is_done: bool):\n",
        "    \"\"\"Generate a training progress table dynamically.\"\"\"\n",
        "    table = Table(title=\"Training Status\")\n",
        "\n",
        "    # Define columns\n",
        "    table.add_column(\"Step\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
        "    table.add_column(\"Epoch\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
        "    table.add_column(\"Learning Rate\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
        "    table.add_column(\"Train Loss\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_column(\"Eval Loss\", justify=\"right\", style=\"green\")\n",
        "    table.add_column(\"Pi Score\", justify=\"right\", style=\"yellow\")\n",
        "\n",
        "    def format_num(num: float | None, digits: int = 4) -> str:\n",
        "        if num is None:\n",
        "            return \"X\"\n",
        "        return format(num, f\".{digits}f\")\n",
        "\n",
        "    for step, data in training_data.items():\n",
        "        table.add_row(\n",
        "            str(step),\n",
        "            format_num(data.get(\"epoch\", None)),\n",
        "            format_num(data.get(\"learning_rate\", None)),\n",
        "            format_num(data.get(\"loss\", None)),\n",
        "            format_num(data.get(\"eval_loss\", None)),\n",
        "            format_num(data.get(\"contract_score\", None)),\n",
        "        )\n",
        "\n",
        "    if not is_done:\n",
        "        table.add_row(\"...\", \"\", \"\", \"\", \"\", \"\")\n",
        "\n",
        "    return table\n",
        "\n",
        "\n",
        "def stream_response(job_id: str, method):\n",
        "    \"\"\"stream_response streams messages from the provided method\n",
        "\n",
        "    method should be a Pi client object with `retrieve` and `stream_messages`\n",
        "    endpoints.  This is primarily for convenience.\"\"\"\n",
        "\n",
        "    training_data = defaultdict(dict)\n",
        "    is_log_console = False\n",
        "\n",
        "    while True:\n",
        "        response = method.retrieve(job_id=job_id)\n",
        "        if (response.state != \"QUEUED\") and (response.state != \"RUNNING\"):\n",
        "            if response.state == \"DONE\" and not is_log_console:\n",
        "                for line in response.detailed_status:\n",
        "                    try:\n",
        "                        data_dict = json.loads(line)\n",
        "                        training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                console.print(generate_table(training_data, is_done=True))\n",
        "            return response\n",
        "\n",
        "        with method.with_streaming_response.stream_messages(\n",
        "            job_id=job_id, timeout=None\n",
        "        ) as response:\n",
        "            with Live(auto_refresh=True, console=console, refresh_per_second=4) as live:\n",
        "                is_done = False\n",
        "                for line in response.iter_lines():\n",
        "                    if line == \"DONE\":\n",
        "                        is_done = True\n",
        "                    try:\n",
        "                        data_dict = json.loads(line)\n",
        "                        training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    live.update(generate_table(training_data, is_done))\n",
        "                    is_log_console = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load a contract and dataset\n",
        "\n",
        "We have a pre-existing contract you can play with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "tldr_contract = load_contract(\n",
        "    \"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/tldr.json\"\n",
        ")\n",
        "\n",
        "num_examples = 200\n",
        "tldr_data = datasets.load_dataset(\"withpi/tldr\")[\"train\"].select(range(num_examples))\n",
        "\n",
        "print(tldr_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Kick off the job\n",
        "\n",
        "The SFT job internally performs a 90/10 train-test split, which is why the loader is not splitting the input data.\n",
        "\n",
        "This process takes a while, please be patient as a cloud GPU is aquired, fine tuning is performed, and a result is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhJrCijJ8I2n"
      },
      "outputs": [],
      "source": [
        "status = client.model.sft.start_job(\n",
        "    contract=tldr_contract,\n",
        "    examples=[\n",
        "        {\"llm_input\": row[\"prompt\"], \"llm_output\": row[\"completion\"]}\n",
        "        for row in tldr_data\n",
        "    ],\n",
        "    base_sft_model=\"LLAMA_3.2_3B\",\n",
        "    num_train_epochs=5,\n",
        ")\n",
        "\n",
        "print(\"SFT model = {}\".format(response.trained_models[0].model_dump_json(indent=2)))\n",
        "repo_name = response.trained_models[0].hf_model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox7XO4DH0lB8"
      },
      "source": [
        "## Load Model\n",
        "\n",
        "You'll need to acknowledge access to the Llama gated model on Hugging Face and generate a token. You can put this into your notebook secrets under \"HF_TOKEN\" and run the cell below.\n",
        "\n",
        "This will load the model into the local GPU.  It takes a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15Q_I77r0hSH"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from vllm import LLM, SamplingParams\n",
        "from vllm.lora.request import LoRARequest\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "adapter_path = snapshot_download(repo_id=sft_model_name)\n",
        "\n",
        "llm = LLM(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    dtype=\"half\",\n",
        "    max_model_len=2048,\n",
        "    enable_lora=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQr8G4HgBUOE"
      },
      "source": [
        "## Try it out!\n",
        "\n",
        "Now lets just do some local inference with this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BoP51rZ8KUP"
      },
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(temperature=0.6, max_tokens=512)\n",
        "\n",
        "response = llm.chat(\n",
        "    messages=[\n",
        "        {\n",
        "            \"content\": \"SUBREDDIT: r/relationships TITLE: My boyfriend [25M] of a year and I [22F] hardly ever argue, except when we are with his family. I can't figure out why this is happening or how to stop it. Help! POST: My boyfriend and I have a great relationship. We communicate well and our conflict resolution styles seem to compliment one another. We rarely disagree on things, and when we do it's resolved with a simple conversation. Here's the problem: When we visit his parents, we argue constantly. It's really starting to bother me because I can't figure out why it's happening. I know that his parents see this and think there's no way we are happy together (because his mom has told me). His parents tend to bicker fairly often, but that shouldn't impact our communication, should it? It doesn't matter if it's just his parents, or if it's the whole extended family. We just seem to argue so much more in their presence. The only exception being when it's just us with his brother and SIL. I've also noticed that they (brother and SIL) seem to be holding back from arguing with one another while we're with the whole family, and they are not like this when it's just the 4 of us. I don't know if it's the stress of seeing his family - I know that they don't *love* us together (probably because we're always arguing!). Or if the family's communication style impacts us. Or if it's just that I'm stressed and little things bother me, and we can't have our simple resolving conversations while we're there. Any ideas what could be causing it? Or how we can stop it from happening? TL;DR:\",\n",
        "            \"role\": \"user\",\n",
        "        },\n",
        "    ],\n",
        "    sampling_params=sampling_params,\n",
        "    lora_request=LoRARequest(\"adapter\", 1, adapter_path),\n",
        "    use_tqdm=False,\n",
        ")\n",
        "\n",
        "print(response[0].outputs[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7s_nuJsHbM"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "You can load this model onto more powerful hardware than the T4 included in Colab or your own inference provider."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
