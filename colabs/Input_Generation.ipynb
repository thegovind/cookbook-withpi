{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Input_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WithPi Input Generation\n",
        "\n",
        "This colab assumes that you already went through [Contract Creation](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Contract_Creation.ipynb), and now you wish to build a small input set.\n",
        "\n",
        "We will walk through the same `Aesop AI` example, but you can load any contract here. Let's dig in!\n",
        "\n",
        "This should take about **15 minutes**, even if you're unfamiliar with Colab."
      ],
      "metadata": {
        "id": "Bwm4tjdnedp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "9HBxNR2oerzC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXIRVg-sMv-S"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install withpi httpx pandas litellm tqdm\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from withpi import PiClient\n",
        "\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "client = PiClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load contract\n",
        "\n",
        "Load the `Aesop AI` example from Pi Labs cookbooks, or edit below to load a different one.\n"
      ],
      "metadata": {
        "id": "s7RRO3iXjYbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "from withpi.types import Contract\n",
        "\n",
        "resp = httpx.get(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai.json\")\n",
        "\n",
        "aesop_contract = Contract.model_validate_json(resp.content)\n",
        "\n",
        "for dimension in aesop_contract.dimensions:\n",
        "  print(dimension.label)\n",
        "  for sub_dimension in dimension.sub_dimensions:\n",
        "    print(f\"\\t{sub_dimension.description}\")"
      ],
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate an Input Set\n",
        "\n",
        "Given this structured description, let's build a Dataset containing a bunch of plausible moral lessons that could be used to exercise the contract.  This will take about 30 seconds to generate."
      ],
      "metadata": {
        "id": "j1FAoBqU7dwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "\n",
        "data_generation_status = client.data.inputs.generate_seeds(\n",
        "    contract_description=aesop_contract.description,\n",
        "    num_inputs=10,\n",
        ")\n",
        "\n",
        "data_table.enable_dataframe_formatter()\n",
        "df = pd.DataFrame({\"input\": data_generation_status.data})\n",
        "df"
      ],
      "metadata": {
        "id": "4NpCZhP6exgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augment with responses\n",
        "\n",
        "Now run inference with your favorite LLM to generate responses to the default prompt.  We will optimize this later.\n",
        "\n",
        "The below cell uses LiteLLM.  You can get a Gemini key from the left side for free to try it out.  You may need a small amount of money in your account to run to completion."
      ],
      "metadata": {
        "id": "gSmwlrjSWtxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from litellm import completion\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "def generate(prompt: str, pbar) -> str:\n",
        "  messages = [\n",
        "      {\n",
        "          \"content\": aesop_contract.description,\n",
        "          \"role\": \"system\"\n",
        "      },\n",
        "      {\n",
        "          \"content\": prompt,\n",
        "          \"role\": \"user\"\n",
        "      },\n",
        "  ]\n",
        "  result = completion(model=\"gemini/gemini-1.5-flash-8b-latest\",\n",
        "                      messages=messages).choices[0].message.content\n",
        "  pbar.update(1)\n",
        "  return result\n",
        "\n",
        "def do_inference():\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(df))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for index, row in df.iterrows():\n",
        "      futures.append(executor.submit(generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "df[\"output\"] = do_inference()\n",
        "df"
      ],
      "metadata": {
        "id": "ENQHR9XTW3LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the set\n",
        "\n",
        "We will come back to this in a future colab, so it's useful to capture.  Store it as a Parquet table, which you can download."
      ],
      "metadata": {
        "id": "rMTmieU9xPgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "filename = \"aesop_ai_examples.parquet\"\n",
        "df.to_parquet(filename)\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "tpWpFWfEwo4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "Now you can try optimizing your prompt with the [Prompt Optimization](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Prompt_Optimization.ipynb) colab to put this to work."
      ],
      "metadata": {
        "id": "jT7s_nuJsHbM"
      }
    }
  ]
}