{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Input_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ],
      "metadata": {
        "id": "pi-masthead"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# Input Generation\n",
        "\n",
        "There is no Playground associated with this Colab, but it's coming soon!\n",
        "\n",
        "Many techniques require input data to drives evaluation and training, but getting high-quality data can be painful and expensive.\n",
        "\n",
        "Generating this data with AI support can give you a higher quality set with much lower effort.  And it can be done with the same Contract that drives other techniques in Pi!\n",
        "\n",
        "We will walk through the same `Aesop AI` example, but you can load any contract here. Let's dig in!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "pi-setup-markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "execution_count": 4,
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "from google.colab import files, userdata\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "%pip install withpi litellm httpx datasets jinja2 tqdm\n",
        "\n",
        "# Import a bunch of useful libraries for later.\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import datasets\n",
        "import httpx\n",
        "import litellm\n",
        "import jinja2\n",
        "from tqdm.notebook import tqdm\n",
        "from withpi import PiClient\n",
        "from withpi.types import Contract\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.live import Live\n",
        "\n",
        "console = Console()\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "def print_contract(contract: Contract):\n",
        "  \"\"\"print_contract pretty-prints a contract\"\"\"\n",
        "  for dimension in contract.dimensions:\n",
        "    print(dimension.label)\n",
        "    for sub_dimension in dimension.sub_dimensions:\n",
        "      print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "  \"\"\"generate passes the provided system and user prompts into the given model\n",
        "  via LiteLLM\"\"\"\n",
        "  messages = [\n",
        "    {\n",
        "      \"content\": system,\n",
        "      \"role\": \"system\"\n",
        "    },\n",
        "    {\n",
        "      \"content\": user,\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ]\n",
        "  return litellm.completion(model=model,\n",
        "                            messages=messages).choices[0].message.content\n",
        "\n",
        "class printer(str):\n",
        "  \"\"\"printer makes strings with embedded newlines print more nicely\"\"\"\n",
        "  def __repr__(self):\n",
        "    return self\n",
        "def print_response(response: str):\n",
        "  \"\"\"print_response pretty-prints an LLM response, respecting newlines\"\"\"\n",
        "  display(printer(response))\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "  \"\"\"print_scores pretty-prints a Pi Score response as a table.\"\"\"\n",
        "  for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "    print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "    for subdimension_name, subdimension_score in dimension_scores.subdimension_scores.items():\n",
        "      print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "    print(\"\\n\")\n",
        "  print(\"---------------------\")\n",
        "  print(f\"Total score: {pi_scores.total_score}\")\n",
        "\n",
        "def save_file(filename: str, model: str):\n",
        "  \"\"\"save_file offers to download the model with the given filename\"\"\"\n",
        "  Path(filename).write_text(model)\n",
        "  files.download(filename)\n",
        "\n",
        "def load_contract(url: str) -> Contract:\n",
        "  \"\"\"load_contract pulls a Contract JSON blob locally with validation.\"\"\"\n",
        "  resp = httpx.get(url)\n",
        "  return Contract.model_validate_json(resp.content)\n",
        "\n",
        "def load_and_split_dataset(url: str) -> datasets.DatasetDict:\n",
        "  \"\"\"load_and_split_dataset pulls in the Parquet file at url and does a 90/10 split\"\"\"\n",
        "  return datasets.load_dataset('parquet', data_files=url, split=\"train\").train_test_split(test_size=0.1)\n",
        "\n",
        "def do_bulk_inference(dataset, system, model):\n",
        "  \"\"\"do_bulk_inference performs inference on the 'input' column of dataset, using\n",
        "  the provided system prompt.  The model identified will be used via LiteLLM\"\"\"\n",
        "\n",
        "  def do_generate(user, pbar):\n",
        "    result = generate(system, user, model)\n",
        "    pbar.update(1)\n",
        "    return result\n",
        "\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(dataset))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for row in dataset:\n",
        "      futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "def do_bulk_templated_inference(dataset, optimized, model):\n",
        "  \"\"\"do_bulk_templated_inference performs inference on the 'input' column of dataset,\n",
        "  using the provided optimized prompt.  It should be a Jinja2 template as returned\n",
        "  by DSPy\"\"\"\n",
        "  prompt_template = jinja2.Template(optimized)\n",
        "  result_extractor = re.compile(r\".*\\[\\[ ## response ## \\]\\](.*)\\[\\[ ## completed ## \\]\\]\", re.DOTALL)\n",
        "\n",
        "  def do_generate(prompt: str, pbar) -> str:\n",
        "    messages = json.loads(prompt_template.render(input=prompt))\n",
        "    result = litellm.completion(model=model,\n",
        "                                messages=messages).choices[0].message.content\n",
        "\n",
        "    pbar.update(1)\n",
        "    return result_extractor.match(result).group(1)\n",
        "\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(dataset))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for row in dataset:\n",
        "      futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "def generate_table(\n",
        "    job_id: str, training_data: dict, is_done: bool, additional_columns: dict[str, str]\n",
        "):\n",
        "    \"\"\"Generate a training progress table dynamically.\"\"\"\n",
        "    table = Table(title=f\"Training Status for {job_id}\")\n",
        "\n",
        "    # Define columns\n",
        "    table.add_column(\"Step\", justify=\"right\", style=\"cyan\")\n",
        "    table.add_column(\"Epoch\", justify=\"right\", style=\"cyan\")\n",
        "    table.add_column(\"Learning Rate\", justify=\"right\", style=\"cyan\")\n",
        "    table.add_column(\"Train Loss\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_column(\"Eval Loss\", justify=\"right\", style=\"green\")\n",
        "    for header in additional_columns.keys():\n",
        "        table.add_column(header, justify=\"right\", style=\"black\")\n",
        "\n",
        "    def format_num(num: float | None, digits: int = 4) -> str:\n",
        "        if num is None:\n",
        "            return \"X\"\n",
        "        return format(num, f\".{digits}f\")\n",
        "\n",
        "    for step, data in training_data.items():\n",
        "        additional_columns_data = [\n",
        "            format_num(data.get(column_name, None))\n",
        "            for column_name in additional_columns.values()\n",
        "        ]\n",
        "        table.add_row(\n",
        "            str(step),\n",
        "            format_num(data.get(\"epoch\", None)),\n",
        "            format_num(data.get(\"learning_rate\", None), digits=10),\n",
        "            format_num(data.get(\"loss\", None)),\n",
        "            format_num(data.get(\"eval_loss\", None)),\n",
        "            *additional_columns_data,\n",
        "        )\n",
        "\n",
        "    if not is_done:\n",
        "        table.add_row(\"...\", \"\", \"\", \"\", \"\", \"\")\n",
        "\n",
        "    return table\n",
        "\n",
        "\n",
        "def stream_response(job_id: str, method, additional_columns: dict[str, str]={}):\n",
        "    \"\"\"stream_response streams messages from the provided method\n",
        "\n",
        "    method should be a Pi client object with `retrieve` and `stream_messages`\n",
        "    endpoints.  This is primarily for convenience.\"\"\"\n",
        "\n",
        "    training_data = defaultdict(dict)\n",
        "    is_log_console = False\n",
        "\n",
        "    while True:\n",
        "        response = method.retrieve(job_id=job_id)\n",
        "        if (response.state != \"QUEUED\") and (response.state != \"RUNNING\"):\n",
        "            if response.state == \"DONE\" and not is_log_console:\n",
        "                for line in response.detailed_status:\n",
        "                    try:\n",
        "                        data_dict = json.loads(line)\n",
        "                        training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                console.print(\n",
        "                    generate_table(\n",
        "                        job_id,\n",
        "                        training_data,\n",
        "                        is_done=True,\n",
        "                        additional_columns=additional_columns,\n",
        "                    )\n",
        "                )\n",
        "            return response\n",
        "\n",
        "        with method.with_streaming_response.stream_messages(\n",
        "            job_id=job_id, timeout=None\n",
        "        ) as response:\n",
        "            with Live(auto_refresh=True, console=console, refresh_per_second=4) as live:\n",
        "                is_done = False\n",
        "                for line in response.iter_lines():\n",
        "                    if line == \"DONE\":\n",
        "                        is_done = True\n",
        "                    try:\n",
        "                        data_dict = json.loads(line)\n",
        "                        training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    live.update(\n",
        "                        generate_table(\n",
        "                            job_id,\n",
        "                            training_data,\n",
        "                            is_done,\n",
        "                            additional_columns=additional_columns,\n",
        "                        )\n",
        "                    )\n",
        "                    is_log_console = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load contract\n",
        "\n",
        "Load the `Aesop AI` example from Pi Labs cookbooks, or edit below to load a different one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "aesop_contract = load_contract(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Generate an Input Set\n",
        "\n",
        "Given this structured description, let's build a Dataset containing a bunch of plausible moral lessons that could be used to exercise the contract.  This will take about 30 seconds to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4NpCZhP6exgI"
      },
      "outputs": [],
      "source": [
        "data_generation_status = client.data.inputs.generate_from_seeds.generate(\n",
        "    application_description=aesop_contract.description,\n",
        "    num_inputs_to_generate=10,\n",
        "    seeds=[],\n",
        "    batch_size=3,\n",
        "    num_shots=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stream the messages as the inputs are generated\n",
        "\n",
        "The messages provide detail about what is being done to generate the data."
      ],
      "metadata": {
        "id": "ktYlEezB236H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    job_status = client.data.inputs.generate_from_seeds.retrieve(data_generation_status.job_id)\n",
        "    if job_status.state in [\"ERROR\", \"DONE\"]:\n",
        "        break\n",
        "\n",
        "    # Steam Messages\n",
        "    with client.data.inputs.with_streaming_response.generate_from_seeds.stream_messages(data_generation_status.job_id) as response:\n",
        "        for line in response.iter_lines():\n",
        "            print(line)"
      ],
      "metadata": {
        "id": "6_cX91PLz7S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can also stream the inputs instead as shown in the cell below"
      ],
      "metadata": {
        "id": "wuG3KiVS3FEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with client.data.inputs.with_streaming_response.generate_from_seeds.stream_data(data_generation_status.job_id) as response:\n",
        "    for line in response.iter_lines():\n",
        "        print(f\"Generated Input: {line}\")"
      ],
      "metadata": {
        "id": "jQaxczWF3Jdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take a look at the returned data\n",
        "\n",
        "Take a look at the returned inputs"
      ],
      "metadata": {
        "id": "bLi5JRNz_IUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print all the data now that the job is complete.\n",
        "if job_status.state not in [\"ERROR\", \"DONE\"]:\n",
        "  print(\"Please wait for the job to finish and then run this cell again...\")\n",
        "else:\n",
        "    if job_status.state == \"DONE\":\n",
        "        print(\"Printing all the generated inputs below...\")\n",
        "        assert job_status.data is not None\n",
        "        for input in job_status.data:\n",
        "            print(input)\n",
        "    else:\n",
        "        print(\"Job ended in error\")"
      ],
      "metadata": {
        "id": "DDOM8UIA96M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSmwlrjSWtxl"
      },
      "source": [
        "# Augment with responses\n",
        "\n",
        "Now run inference with your favorite LLM to generate responses to the default prompt.  We will optimize this later.\n",
        "\n",
        "The below cell uses LiteLLM.  You can get a Gemini key from the left side for free to try it out.  You may need a small amount of money in your account to run to completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENQHR9XTW3LP"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "input_set = datasets.Dataset.from_dict({'input': job_status.data})\n",
        "input_set = input_set.add_column(\n",
        "    'output', do_bulk_inference(input_set, aesop_contract.description, \"gemini/gemini-1.5-flash-8b\"))\n",
        "input_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMTmieU9xPgT"
      },
      "source": [
        "## Save the set\n",
        "\n",
        "We will come back to this in a future colab, so it's useful to capture.  Store it as a Parquet table, which you can download.\n",
        "\n",
        "Alternatively, upload to Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpWpFWfEwo4j"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "filename = \"aesop_ai_examples.parquet\"\n",
        "input_set.to_parquet(filename)\n",
        "files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7s_nuJsHbM"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "This input set can drive many other techniques in Pi.  You can adjust the above methods to add seeds and steer the AI in different ways."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}