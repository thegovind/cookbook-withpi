{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Prompt_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ],
      "metadata": {
        "id": "pi-masthead"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# Prompt Optimization with Scorer\n",
        "\n",
        "This Colab is the companion to the \"Prompt optimization with scorers and diff view\" Playground, which introduces the core concept of Pi, the **Contract**.\n",
        "\n",
        "A **Contract** is a **human and machine readable** description of what **goodness** means to you and is the cornerstone of our approach because it lets you measure improvements mechanically, while still being explainable.  See [Key Concepts](https://code.withpi.ai/key-concepts) for more information.\n",
        "\n",
        "This colab will walk you through generating a **Contract**, scoring some responses with it, and tinkering with your application description to improve it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "pi-setup-markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "%%capture\n",
        "\n",
        "%pip install withpi litellm httpx datasets jinja2 tqdm\n",
        "\n",
        "# Import a bunch of useful libraries for later.\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import datasets\n",
        "from google.colab import files, userdata\n",
        "import httpx\n",
        "import jinja2\n",
        "from litellm import completion\n",
        "from tqdm.notebook import tqdm\n",
        "from withpi import PiClient\n",
        "from withpi.types import Contract\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "def print_contract(contract: Contract):\n",
        "  \"\"\"print_contract pretty-prints a contract\"\"\"\n",
        "  for dimension in contract.dimensions:\n",
        "    print(dimension.label)\n",
        "    for sub_dimension in dimension.sub_dimensions:\n",
        "      print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "  \"\"\"generate passes the provided system and user prompts into the given model\n",
        "  via LiteLLM\"\"\"\n",
        "  messages = [\n",
        "    {\n",
        "      \"content\": system,\n",
        "      \"role\": \"system\"\n",
        "    },\n",
        "    {\n",
        "      \"content\": user,\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ]\n",
        "  return completion(model=model,\n",
        "                    messages=messages).choices[0].message.content\n",
        "\n",
        "class printer(str):\n",
        "  \"\"\"printer makes strings with embedded newlines print more nicely\"\"\"\n",
        "  def __repr__(self):\n",
        "    return self\n",
        "def print_response(response: str):\n",
        "  \"\"\"print_response pretty-prints an LLM response, respecting newlines\"\"\"\n",
        "  display(printer(response))\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "  \"\"\"print_scores pretty-prints a Pi Score response as a table.\"\"\"\n",
        "  for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "    print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "    for subdimension_name, subdimension_score in dimension_scores.subdimension_scores.items():\n",
        "      print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "    print(\"\\n\")\n",
        "  print(\"---------------------\")\n",
        "  print(f\"Total score: {pi_scores.total_score}\")\n",
        "\n",
        "def save_file(filename: str, model: str):\n",
        "  \"\"\"save_file offers to download the model with the given filename\"\"\"\n",
        "  Path(filename).write_text(model)\n",
        "  files.download(filename)\n",
        "\n",
        "def load_contract(url: str) -> Contract:\n",
        "  \"\"\"load_contract pulls a Contract JSON blob locally with validation.\"\"\"\n",
        "  resp = httpx.get(url)\n",
        "  return Contract.model_validate_json(resp.content)\n",
        "\n",
        "def load_and_split_dataset(url: str) -> datasets.DatasetDict:\n",
        "  \"\"\"load_and_split_dataset pulls in the Parquet file at url and does a 90/10 split\"\"\"\n",
        "  return datasets.load_dataset('parquet', data_files=url, split=\"train\").train_test_split(test_size=0.1)\n",
        "\n",
        "def do_bulk_inference(dataset, system, model):\n",
        "  \"\"\"do_bulk_inference performs inference on the 'input' column of dataset, using\n",
        "  the provided system prompt.  The model identified will be used via LiteLLM\"\"\"\n",
        "\n",
        "  def do_generate(user, pbar):\n",
        "    result = generate(system, user, model)\n",
        "    pbar.update(1)\n",
        "    return result\n",
        "\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(dataset))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for row in dataset:\n",
        "      futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "def do_bulk_templated_inference(dataset, optimized, model):\n",
        "  \"\"\"do_bulk_templated_inference performs inference on the 'input' column of dataset,\n",
        "  using the provided optimized prompt.  It should be a Jinja2 template as returned\n",
        "  by DSPy\"\"\"\n",
        "  prompt_template = jinja2.Template(optimized)\n",
        "  result_extractor = re.compile(r\".*\\[\\[ ## response ## \\]\\](.*)\\[\\[ ## completed ## \\]\\]\", re.DOTALL)\n",
        "\n",
        "  def do_generate(prompt: str, pbar) -> str:\n",
        "    messages = json.loads(prompt_template.render(input=prompt))\n",
        "    result = completion(model=model,\n",
        "                        messages=messages).choices[0].message.content\n",
        "\n",
        "    pbar.update(1)\n",
        "    return result_extractor.match(result).group(1)\n",
        "\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(dataset))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for row in dataset:\n",
        "      futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "def stream_response(job_id: str, method):\n",
        "  \"\"\"stream_response streams messages from the provided method\n",
        "\n",
        "  method should be a Pi client object with `retrieve` and `stream_messages`\n",
        "  endpoints.  This is primarily for convenience.\"\"\"\n",
        "\n",
        "  while True:\n",
        "    response = method.retrieve(job_id=job_id)\n",
        "    if (response.state != 'QUEUED') and (response.state != 'RUNNING'):\n",
        "      return response\n",
        "\n",
        "    with method.with_streaming_response.stream_messages(\n",
        "        job_id=job_id, timeout=None) as response:\n",
        "      for line in response.iter_lines():\n",
        "        print(line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Make a contract\n",
        "\n",
        "Let's say you want to build an application that generates children's stories teaching a life lesson.  Call it `AesopAI`.\n",
        "\n",
        "Start by creating a first cut contract based on that general input, proposed in the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "aesop_contract = client.contracts.generate_dimensions(\n",
        "    contract_description=(\n",
        "        \"Write a children's story in the style of Aesop's Fables \"\n",
        "        \"teaching a life lesson specified by the user. Provide just the \"\n",
        "        \"story with no extra content.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print_contract(aesop_contract)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4eChTjc66_f"
      },
      "source": [
        "A contract is essentially a hierarchical rubric for grading a response.  A bunch of \"simple\" questions add up to broader categories, which yield a final score.  Output will vary somewhat, but the table above should have reasonable grading questions for the application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Score the contract\n",
        "\n",
        "Let's see how it performs! The below cell uses Gemini to generate a response, but any suitable model will work fine.\n",
        "\n",
        "Adjust to pick a different model and supply your own key with docs at https://docs.litellm.ai/docs/.\n",
        "\n",
        "You can import a Google Gemini key from AI Studio on the left pane, which populates a GOOGLE_API_KEY secret.  At low rates it's free."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhJrCijJ8I2n"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "prompt = \"The importance of sharing\"\n",
        "response = generate(system=aesop_contract.description, user=prompt, model=\"gemini/gemini-1.5-flash-8b\")\n",
        "\n",
        "print_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU0TJGzFk-2C"
      },
      "source": [
        "## Score it!\n",
        "\n",
        "Take the generated response and see how it scores with Pi.\n",
        "\n",
        "The below cell will run Pi Scoring, evaluating each dimension in the contract, offering a score from 1 (excellent!) to 0 (terrible!).  The current contract is **uncalibrated**, meaning that all the dimensions are equally important, but it's a starting point for learning which are **actually** imporant based on your preferences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NpCZhP6exgI"
      },
      "outputs": [],
      "source": [
        "pi_scores = client.contracts.score(\n",
        "    contract=aesop_contract,\n",
        "    llm_input=prompt,\n",
        "    llm_output=response,\n",
        ")\n",
        "\n",
        "print_scores(pi_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9UOAOSQn1bF"
      },
      "source": [
        "## Save it!\n",
        "\n",
        "Finally, save the Contract so you can come back to it later.\n",
        "\n",
        "A contract is a simple Pydantic model, which can be serialized to JSON and stored locally.\n",
        "\n",
        "The cell below will offer a download of the contract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPlsGumImqm1"
      },
      "outputs": [],
      "source": [
        "save_file('aesop_ai.json', aesop_contract.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7s_nuJsHbM"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Go back and try different system prompts to see how they respond to outputs.  Try a different model.  Manually tweak the dimensions. Get a feel for what's happening.\n",
        "\n",
        "When you're ready to move beyond basic vibe checking, you'll need to take a systematic approach.  To do that, you'll need input data.  Fortunately, we have tools to help build a representative set.  Head over to the input data playground for this."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}