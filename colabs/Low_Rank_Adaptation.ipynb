{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Low_Rank_Adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ],
      "metadata": {
        "id": "pi-masthead"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# Low Rank Adaptation\n",
        "\n",
        "This is the companion to the Low Rank Adaptation playground\n",
        "\n",
        "Assuming you have some \"golden\" responses, this helps you fine tune an adapter that helps a small model align more with the preference expressed in those responses."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "This notebook needs a T4 GPU.  Make sure to select this explicitly above before proceeding.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "pi-setup-markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "from google.colab import files, userdata\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "%pip install withpi litellm httpx datasets jinja2 tqdm\n",
        "\n",
        "# Import a bunch of useful libraries for later.\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import datasets\n",
        "import httpx\n",
        "import jinja2\n",
        "from litellm import completion\n",
        "from tqdm.notebook import tqdm\n",
        "from withpi import PiClient\n",
        "from withpi.types import Contract\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "def print_contract(contract: Contract):\n",
        "  \"\"\"print_contract pretty-prints a contract\"\"\"\n",
        "  for dimension in contract.dimensions:\n",
        "    print(dimension.label)\n",
        "    for sub_dimension in dimension.sub_dimensions:\n",
        "      print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "  \"\"\"generate passes the provided system and user prompts into the given model\n",
        "  via LiteLLM\"\"\"\n",
        "  messages = [\n",
        "    {\n",
        "      \"content\": system,\n",
        "      \"role\": \"system\"\n",
        "    },\n",
        "    {\n",
        "      \"content\": user,\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ]\n",
        "  return completion(model=model,\n",
        "                    messages=messages).choices[0].message.content\n",
        "\n",
        "class printer(str):\n",
        "  \"\"\"printer makes strings with embedded newlines print more nicely\"\"\"\n",
        "  def __repr__(self):\n",
        "    return self\n",
        "def print_response(response: str):\n",
        "  \"\"\"print_response pretty-prints an LLM response, respecting newlines\"\"\"\n",
        "  display(printer(response))\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "  \"\"\"print_scores pretty-prints a Pi Score response as a table.\"\"\"\n",
        "  for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "    print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "    for subdimension_name, subdimension_score in dimension_scores.subdimension_scores.items():\n",
        "      print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "    print(\"\\n\")\n",
        "  print(\"---------------------\")\n",
        "  print(f\"Total score: {pi_scores.total_score}\")\n",
        "\n",
        "def save_file(filename: str, model: str):\n",
        "  \"\"\"save_file offers to download the model with the given filename\"\"\"\n",
        "  Path(filename).write_text(model)\n",
        "  files.download(filename)\n",
        "\n",
        "def load_contract(url: str) -> Contract:\n",
        "  \"\"\"load_contract pulls a Contract JSON blob locally with validation.\"\"\"\n",
        "  resp = httpx.get(url)\n",
        "  return Contract.model_validate_json(resp.content)\n",
        "\n",
        "def load_and_split_dataset(url: str) -> datasets.DatasetDict:\n",
        "  \"\"\"load_and_split_dataset pulls in the Parquet file at url and does a 90/10 split\"\"\"\n",
        "  return datasets.load_dataset('parquet', data_files=url, split=\"train\").train_test_split(test_size=0.1)\n",
        "\n",
        "def do_bulk_inference(dataset, system, model):\n",
        "  \"\"\"do_bulk_inference performs inference on the 'input' column of dataset, using\n",
        "  the provided system prompt.  The model identified will be used via LiteLLM\"\"\"\n",
        "\n",
        "  def do_generate(user, pbar):\n",
        "    result = generate(system, user, model)\n",
        "    pbar.update(1)\n",
        "    return result\n",
        "\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(dataset))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for row in dataset:\n",
        "      futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "def do_bulk_templated_inference(dataset, optimized, model):\n",
        "  \"\"\"do_bulk_templated_inference performs inference on the 'input' column of dataset,\n",
        "  using the provided optimized prompt.  It should be a Jinja2 template as returned\n",
        "  by DSPy\"\"\"\n",
        "  prompt_template = jinja2.Template(optimized)\n",
        "  result_extractor = re.compile(r\".*\\[\\[ ## response ## \\]\\](.*)\\[\\[ ## completed ## \\]\\]\", re.DOTALL)\n",
        "\n",
        "  def do_generate(prompt: str, pbar) -> str:\n",
        "    messages = json.loads(prompt_template.render(input=prompt))\n",
        "    result = completion(model=model,\n",
        "                        messages=messages).choices[0].message.content\n",
        "\n",
        "    pbar.update(1)\n",
        "    return result_extractor.match(result).group(1)\n",
        "\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(dataset))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for row in dataset:\n",
        "      futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "def stream_response(job_id: str, method):\n",
        "  \"\"\"stream_response streams messages from the provided method\n",
        "\n",
        "  method should be a Pi client object with `retrieve` and `stream_messages`\n",
        "  endpoints.  This is primarily for convenience.\"\"\"\n",
        "\n",
        "  while True:\n",
        "    response = method.retrieve(job_id=job_id)\n",
        "    if (response.state != 'QUEUED') and (response.state != 'RUNNING'):\n",
        "      return response\n",
        "\n",
        "    with method.with_streaming_response.stream_messages(\n",
        "        job_id=job_id, timeout=None) as response:\n",
        "      for line in response.iter_lines():\n",
        "        print(line)\n",
        "\n",
        "%pip install vllm huggingface_hub bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load a contract and dataset\n",
        "\n",
        "We have a pre-existing contract you can play with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "aesop_contract = load_contract(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai.json\")\n",
        "aesop = datasets.load_dataset('parquet', data_files=\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/datasets/aesop_ai_examples.parquet\", split=\"train\")\n",
        "aesop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Kick off the job\n",
        "\n",
        "The SFT job internally performs a 90/10 train-test split, which is why the loader is not splitting the input data.\n",
        "\n",
        "This process takes a while, please be patient as a cloud GPU is aquired, fine tuning is performed, and a result is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhJrCijJ8I2n"
      },
      "outputs": [],
      "source": [
        "status = client.model.sft.start_job(\n",
        "    contract=aesop_contract,\n",
        "    examples=[{'llm_input': row['input'], 'llm_output': row['output']} for row in aesop],\n",
        "    base_sft_model='LLAMA_3.2_3B',\n",
        "    num_train_epochs=1\n",
        ")\n",
        "\n",
        "response = stream_response(status.job_id, client.model.sft)\n",
        "repo_name = response.trained_models[0].hf_model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model\n",
        "\n",
        "You'll need to acknowledge access to the Llama gated model on Hugging Face and generate a token. You can put this into your notebook secrets under \"HF_TOKEN\" and run the cell below.\n",
        "\n",
        "This will load the model into the local GPU.  It takes a few minutes."
      ],
      "metadata": {
        "id": "Ox7XO4DH0lB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "adapter_path = snapshot_download(repo_id=repo_name)\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "from vllm.lora.request import LoRARequest\n",
        "\n",
        "llm = LLM(model=\"meta-llama/Llama-3.2-3B-Instruct\", dtype='half', max_model_len=2048, enable_lora=True)"
      ],
      "metadata": {
        "id": "15Q_I77r0hSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try it out!\n",
        "\n",
        "Now lets just do some local inference with this model."
      ],
      "metadata": {
        "id": "JQr8G4HgBUOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=0,\n",
        "    max_tokens=512,\n",
        "    stop=[\"[/assistant]\"]\n",
        ")\n",
        "\n",
        "response = llm.chat(\n",
        "    messages = [\n",
        "      {\n",
        "        \"content\": aesop_contract.description,\n",
        "        \"role\": \"system\"\n",
        "      },\n",
        "      {\n",
        "        \"content\": \"The importance of sharing\",\n",
        "        \"role\": \"user\"\n",
        "      }\n",
        "    ],\n",
        "    sampling_params=sampling_params,\n",
        "    lora_request=LoRARequest(\"adapter\", 1, adapter_path),\n",
        "    use_tqdm=False\n",
        ")\n",
        "\n",
        "print_response(response[0].outputs[0].text)"
      ],
      "metadata": {
        "id": "-BoP51rZ8KUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7s_nuJsHbM"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "You can load this model onto more powerful hardware than the T4 included in Colab or your own inference provider."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}