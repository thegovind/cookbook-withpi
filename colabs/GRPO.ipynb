{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ],
      "metadata": {
        "id": "pi-masthead"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# Reinforcement Learning GRPO\n",
        "\n",
        "This is the companion to the RL playground\n",
        "\n",
        "Description: Train models to more deeply learn patterns from your data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "pi-setup-markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "from google.colab import files, userdata\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "%pip install withpi litellm httpx datasets jinja2 tqdm\n",
        "\n",
        "# Import a bunch of useful libraries for later.\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import datasets\n",
        "import httpx\n",
        "import litellm\n",
        "import jinja2\n",
        "from tqdm.notebook import tqdm\n",
        "from withpi import PiClient\n",
        "from withpi.types import Contract\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.live import Live\n",
        "\n",
        "console = Console()\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "def print_contract(contract: Contract):\n",
        "  \"\"\"print_contract pretty-prints a contract\"\"\"\n",
        "  for dimension in contract.dimensions:\n",
        "    print(dimension.label)\n",
        "    for sub_dimension in dimension.sub_dimensions:\n",
        "      print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "  \"\"\"generate passes the provided system and user prompts into the given model\n",
        "  via LiteLLM\"\"\"\n",
        "  messages = [\n",
        "    {\n",
        "      \"content\": system,\n",
        "      \"role\": \"system\"\n",
        "    },\n",
        "    {\n",
        "      \"content\": user,\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ]\n",
        "  return litellm.completion(model=model,\n",
        "                            messages=messages).choices[0].message.content\n",
        "\n",
        "class printer(str):\n",
        "  \"\"\"printer makes strings with embedded newlines print more nicely\"\"\"\n",
        "  def __repr__(self):\n",
        "    return self\n",
        "def print_response(response: str):\n",
        "  \"\"\"print_response pretty-prints an LLM response, respecting newlines\"\"\"\n",
        "  display(printer(response))\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "  \"\"\"print_scores pretty-prints a Pi Score response as a table.\"\"\"\n",
        "  for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "    print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "    for subdimension_name, subdimension_score in dimension_scores.subdimension_scores.items():\n",
        "      print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "    print(\"\\n\")\n",
        "  print(\"---------------------\")\n",
        "  print(f\"Total score: {pi_scores.total_score}\")\n",
        "\n",
        "def save_file(filename: str, model: str):\n",
        "  \"\"\"save_file offers to download the model with the given filename\"\"\"\n",
        "  Path(filename).write_text(model)\n",
        "  files.download(filename)\n",
        "\n",
        "def load_contract(url: str) -> Contract:\n",
        "  \"\"\"load_contract pulls a Contract JSON blob locally with validation.\"\"\"\n",
        "  resp = httpx.get(url)\n",
        "  return Contract.model_validate_json(resp.content)\n",
        "\n",
        "def load_and_split_dataset(url: str) -> datasets.DatasetDict:\n",
        "  \"\"\"load_and_split_dataset pulls in the Parquet file at url and does a 90/10 split\"\"\"\n",
        "  return datasets.load_dataset('parquet', data_files=url, split=\"train\").train_test_split(test_size=0.1)\n",
        "\n",
        "def do_bulk_inference(dataset, system, model):\n",
        "  \"\"\"do_bulk_inference performs inference on the 'input' column of dataset, using\n",
        "  the provided system prompt.  The model identified will be used via LiteLLM\"\"\"\n",
        "\n",
        "  def do_generate(user, pbar):\n",
        "    result = generate(system, user, model)\n",
        "    pbar.update(1)\n",
        "    return result\n",
        "\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(dataset))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for row in dataset:\n",
        "      futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "def do_bulk_templated_inference(dataset, optimized, model):\n",
        "  \"\"\"do_bulk_templated_inference performs inference on the 'input' column of dataset,\n",
        "  using the provided optimized prompt.  It should be a Jinja2 template as returned\n",
        "  by DSPy\"\"\"\n",
        "  prompt_template = jinja2.Template(optimized)\n",
        "  result_extractor = re.compile(r\".*\\[\\[ ## response ## \\]\\](.*)\\[\\[ ## completed ## \\]\\]\", re.DOTALL)\n",
        "\n",
        "  def do_generate(prompt: str, pbar) -> str:\n",
        "    messages = json.loads(prompt_template.render(input=prompt))\n",
        "    result = litellm.completion(model=model,\n",
        "                                messages=messages).choices[0].message.content\n",
        "\n",
        "    pbar.update(1)\n",
        "    return result_extractor.match(result).group(1)\n",
        "\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(dataset))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for row in dataset:\n",
        "      futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "def generate_table(\n",
        "    job_id: str, training_data: dict, is_done: bool, additional_columns: dict[str, str]\n",
        "):\n",
        "    \"\"\"Generate a training progress table dynamically.\"\"\"\n",
        "    table = Table(title=f\"Training Status for {job_id}\")\n",
        "\n",
        "    # Define columns\n",
        "    table.add_column(\"Step\", justify=\"right\", style=\"cyan\")\n",
        "    table.add_column(\"Epoch\", justify=\"right\", style=\"cyan\")\n",
        "    table.add_column(\"Learning Rate\", justify=\"right\", style=\"cyan\")\n",
        "    table.add_column(\"Train Loss\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_column(\"Eval Loss\", justify=\"right\", style=\"green\")\n",
        "    for header in additional_columns.keys():\n",
        "        table.add_column(header, justify=\"right\", style=\"black\")\n",
        "\n",
        "    def format_num(num: float | None, digits: int = 4) -> str:\n",
        "        if num is None:\n",
        "            return \"X\"\n",
        "        return format(num, f\".{digits}f\")\n",
        "\n",
        "    for step, data in training_data.items():\n",
        "        additional_columns_data = [\n",
        "            format_num(data.get(column_name, None))\n",
        "            for column_name in additional_columns.values()\n",
        "        ]\n",
        "        table.add_row(\n",
        "            str(step),\n",
        "            format_num(data.get(\"epoch\", None)),\n",
        "            format_num(data.get(\"learning_rate\", None), digits=10),\n",
        "            format_num(data.get(\"loss\", None)),\n",
        "            format_num(data.get(\"eval_loss\", None)),\n",
        "            *additional_columns_data,\n",
        "        )\n",
        "\n",
        "    if not is_done:\n",
        "        table.add_row(\"...\", \"\", \"\", \"\", \"\", \"\")\n",
        "\n",
        "    return table\n",
        "\n",
        "\n",
        "def stream_response(job_id: str, method, additional_columns: dict[str, str]):\n",
        "    \"\"\"stream_response streams messages from the provided method\n",
        "\n",
        "    method should be a Pi client object with `retrieve` and `stream_messages`\n",
        "    endpoints.  This is primarily for convenience.\"\"\"\n",
        "\n",
        "    training_data = defaultdict(dict)\n",
        "    is_log_console = False\n",
        "\n",
        "    while True:\n",
        "        response = method.retrieve(job_id=job_id)\n",
        "        if (response.state != \"QUEUED\") and (response.state != \"RUNNING\"):\n",
        "            if response.state == \"DONE\" and not is_log_console:\n",
        "                for line in response.detailed_status:\n",
        "                    try:\n",
        "                        data_dict = json.loads(line)\n",
        "                        training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                console.print(\n",
        "                    generate_table(\n",
        "                        job_id,\n",
        "                        training_data,\n",
        "                        is_done=True,\n",
        "                        additional_columns=additional_columns,\n",
        "                    )\n",
        "                )\n",
        "            return response\n",
        "\n",
        "        with method.with_streaming_response.stream_messages(\n",
        "            job_id=job_id, timeout=None\n",
        "        ) as response:\n",
        "            with Live(auto_refresh=True, console=console, refresh_per_second=4) as live:\n",
        "                is_done = False\n",
        "                for line in response.iter_lines():\n",
        "                    if line == \"DONE\":\n",
        "                        is_done = True\n",
        "                    try:\n",
        "                        data_dict = json.loads(line)\n",
        "                        training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    live.update(\n",
        "                        generate_table(\n",
        "                            job_id,\n",
        "                            training_data,\n",
        "                            is_done,\n",
        "                            additional_columns=additional_columns,\n",
        "                        )\n",
        "                    )\n",
        "                    is_log_console = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load a contract and dataset\n",
        "\n",
        "We have a pre-existing contract you can play with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "tldr_contract = load_contract(\n",
        "    \"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/tldr.json\"\n",
        ")\n",
        "\n",
        "num_examples = 100\n",
        "tldr_data = datasets.load_dataset(\"withpi/tldr\")[\"train\"].select(range(num_examples))\n",
        "\n",
        "print(tldr_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Kick off the job\n",
        "\n",
        "The GRPO job internally performs a 90/10 train-test split, which is why the loader is not splitting the input data.\n",
        "\n",
        "This process takes a while, please be patient as a cloud GPU is aquired, fine tuning is performed, and a result is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvhTP20ijfNO"
      },
      "outputs": [],
      "source": [
        "status = client.model.rl.grpo.start_job(\n",
        "    contract=tldr_contract,\n",
        "    examples=[{\"llm_input\": row[\"prompt\"]} for row in tldr_data],\n",
        "    model=\"LLAMA_3.2_1B\",\n",
        "    num_train_epochs=1,\n",
        ")\n",
        "print(status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "WhJrCijJ8I2n",
        "outputId": "65d126be-9157-410a-bf5a-a7fbf68c5ba8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                       Training Status for rl_grpo_jobs:086f516d-b9f7-4585-82fc-923f75ddca70                       \u001b[0m\n",
              "┏━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1m      \u001b[0m┃\u001b[1m        \u001b[0m┃\u001b[1m        \u001b[0m┃\u001b[1m        \u001b[0m┃\u001b[1m        \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Train\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Train\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Eval\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Eval\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m        \u001b[0m┃\u001b[1m        \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Train\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Eval\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┃\u001b[1m      \u001b[0m┃\u001b[1m        \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLearn…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Train\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Eval\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Pi\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Std\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Pi\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Std\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Train\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Eval\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCompl…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mComp…\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┃\u001b[1m \u001b[0m\u001b[1mStep\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Epoch\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Rate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Loss\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Loss\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mReward\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mReward\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mReward\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mReward\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    KL\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    KL\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLength\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLeng…\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m 100\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m0.4444\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m0.000…\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.0020\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     X\u001b[0m\u001b[32m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.7826\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.0538\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m     X\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m     X\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.0500\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m     X\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m353.1…\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m    X\u001b[0m\u001b[30m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m 200\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m0.8889\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m0.000…\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.0015\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0006\u001b[0m\u001b[32m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.7782\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.0598\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.7918\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.0563\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.0372\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.0157\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m353.2…\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m380.…\u001b[0m\u001b[30m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m 225\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m1.0000\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m     X\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     X\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     X\u001b[0m\u001b[32m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.7894\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.0561\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m     X\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m     X\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m0.0202\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m     X\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m373.4…\u001b[0m\u001b[30m \u001b[0m│\u001b[30m \u001b[0m\u001b[30m    X\u001b[0m\u001b[30m \u001b[0m│\n",
              "└──────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴───────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Training Status for rl_grpo_jobs:086f516d-b9f7-4585-82fc-923f75ddca70                       </span>\n",
              "┏━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">      </span>┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">  Train </span>┃<span style=\"font-weight: bold\">  Train </span>┃<span style=\"font-weight: bold\">   Eval </span>┃<span style=\"font-weight: bold\">   Eval </span>┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">  Train </span>┃<span style=\"font-weight: bold\">  Eval </span>┃\n",
              "┃<span style=\"font-weight: bold\">      </span>┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\"> Learn… </span>┃<span style=\"font-weight: bold\">  Train </span>┃<span style=\"font-weight: bold\">   Eval </span>┃<span style=\"font-weight: bold\">     Pi </span>┃<span style=\"font-weight: bold\">    Std </span>┃<span style=\"font-weight: bold\">     Pi </span>┃<span style=\"font-weight: bold\">    Std </span>┃<span style=\"font-weight: bold\">  Train </span>┃<span style=\"font-weight: bold\">   Eval </span>┃<span style=\"font-weight: bold\"> Compl… </span>┃<span style=\"font-weight: bold\"> Comp… </span>┃\n",
              "┃<span style=\"font-weight: bold\"> Step </span>┃<span style=\"font-weight: bold\">  Epoch </span>┃<span style=\"font-weight: bold\">   Rate </span>┃<span style=\"font-weight: bold\">   Loss </span>┃<span style=\"font-weight: bold\">   Loss </span>┃<span style=\"font-weight: bold\"> Reward </span>┃<span style=\"font-weight: bold\"> Reward </span>┃<span style=\"font-weight: bold\"> Reward </span>┃<span style=\"font-weight: bold\"> Reward </span>┃<span style=\"font-weight: bold\">     KL </span>┃<span style=\"font-weight: bold\">     KL </span>┃<span style=\"font-weight: bold\"> Length </span>┃<span style=\"font-weight: bold\"> Leng… </span>┃\n",
              "┡━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">  100 </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> 0.4444 </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> 0.000… </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.0020 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      X </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.7826 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.0538 </span>│<span style=\"color: #000000; text-decoration-color: #000000\">      X </span>│<span style=\"color: #000000; text-decoration-color: #000000\">      X </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.0500 </span>│<span style=\"color: #000000; text-decoration-color: #000000\">      X </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 353.1… </span>│<span style=\"color: #000000; text-decoration-color: #000000\">     X </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">  200 </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> 0.8889 </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> 0.000… </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.0015 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0006 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.7782 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.0598 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.7918 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.0563 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.0372 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.0157 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 353.2… </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 380.… </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">  225 </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> 1.0000 </span>│<span style=\"color: #008080; text-decoration-color: #008080\">      X </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      X </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      X </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.7894 </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.0561 </span>│<span style=\"color: #000000; text-decoration-color: #000000\">      X </span>│<span style=\"color: #000000; text-decoration-color: #000000\">      X </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 0.0202 </span>│<span style=\"color: #000000; text-decoration-color: #000000\">      X </span>│<span style=\"color: #000000; text-decoration-color: #000000\"> 373.4… </span>│<span style=\"color: #000000; text-decoration-color: #000000\">     X </span>│\n",
              "└──────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴───────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRPO model = {\n",
            "  \"contract_score\": 0.7917511820793152,\n",
            "  \"epoch\": 0.8888888888888888,\n",
            "  \"eval_loss\": 0.000626887078396976,\n",
            "  \"firework_hosted_model_id\": \"\",\n",
            "  \"step\": 200,\n",
            "  \"hf_model_name\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "response = stream_response(\n",
        "    status.job_id,\n",
        "    client.model.rl.grpo,\n",
        "    additional_columns={\n",
        "        \"Train Pi Reward\": \"rewards/pi_reward_func\",\n",
        "        \"Train Std Reward\": \"reward_std\",\n",
        "        \"Eval Pi Reward\": \"eval_rewards/pi_reward_func\",\n",
        "        \"Eval Std Reward\": \"eval_reward_std\",\n",
        "        \"Train KL\": \"kl\",\n",
        "        \"Eval KL\": \"eval_kl\",\n",
        "        \"Train Completion Length\": \"completion_length\",\n",
        "        \"Eval Completion Length\": \"eval_completion_length\",\n",
        "    },\n",
        ")\n",
        "print(\"GRPO model = {}\".format(response.trained_models[0].model_dump_json(indent=2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now load the model\n",
        "\n",
        "Load the model into a serving cluster"
      ],
      "metadata": {
        "id": "jnOUGxaoe-L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.model.rl.grpo.load(\n",
        "    status.job_id,\n",
        ")\n",
        "\n",
        "for idx in range(200):\n",
        "  is_done = client.model.rl.grpo.check(\n",
        "      status.job_id,\n",
        "  )\n",
        "  if is_done:\n",
        "    print(\"Loaded!\")\n",
        "    break\n",
        "  else:\n",
        "    time.sleep(3)\n",
        "if not is_done:\n",
        "  print(\"Did not load in time.\")"
      ],
      "metadata": {
        "id": "aFizTaKyfAvz",
        "outputId": "e7dbcfe8-81a7-4202-bef5-f941b1edf2b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query your model\n",
        "\n",
        "The below cell will query your model."
      ],
      "metadata": {
        "id": "ANYANbJFf2LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"SUBREDDIT: r/relationships TITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting POST: Not sure if this belongs here but it's worth a try. Backstory: When I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. Now: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. His friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. So I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions. TL;DR:\"\"\"\n",
        "\n",
        "response = litellm.text_completion(\n",
        "    prompt=prompt,\n",
        "    model=\"fireworks_ai/unused\",\n",
        "    api_base=f\"https://api.withpi.ai/v1/model/rl/grpo/{status.job_id}\",\n",
        "    api_key=os.environ[\"WITHPI_API_KEY\"],\n",
        "    max_tokens=2048,\n",
        ")\n",
        "\n",
        "print(\"Raw Completion response:\\n\")\n",
        "print_response(response.choices[0].text)\n",
        "\n",
        "response = litellm.completion(\n",
        "    messages=[\n",
        "        {\"content\": prompt,\n",
        "         \"role\": \"user\",\n",
        "    }],\n",
        "    model=\"fireworks_ai/unused\",\n",
        "    api_base=f\"https://api.withpi.ai/v1/model/rl/grpo/{status.job_id}\",\n",
        "    api_key=os.environ[\"WITHPI_API_KEY\"],\n",
        "    max_tokens=2048\n",
        ")\n",
        "print(\"\\nChat completion:\\n\")\n",
        "print_response(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "rhAWye6Df5FT",
        "outputId": "68f75f00-64bd-4518-e444-a178ecc62f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Completion response:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " Going through a tough breakup and still running into people from that time. Trying to figure out best course of action to navigate that, whether to cut ties or stay in touch and still somewhat know him.\n",
              "\n",
              "I provided a real-life scenario to illustrate the question for better understanding. I acknowledge that i'm sharing a personal struggle and not trying to imply that they're also in my situation, but it shows the effort and the fact that many people experience certain challenges in their relationships.\n",
              "\n",
              "**I've marked this post as such because some of the comments have asked more serious and nuanced questions so I may not see this in the future, which makes me think others may ask this same question as well**. So I figured people understood it best here. Given the importance it has to this person and the fact that people have experiences similar to theirs. I thank each of you for reading this, taking the time to respond.\n",
              "\n",
              "(10 replies) \n",
              "\n",
              "Here are the brief summaries of the comments:\n",
              "\n",
              "### General Understandings\n",
              "1.  The OP asked for advice on whether to cut ties with people from her past in a relationship. This is a common problem associated with unresolved grief and attachment issues.\n",
              "2.  There are multiple reasons why someone might want to remain connected with these people, such as maintaining emotional connection or just for friendship.\n",
              "3.  Some people suggest seeking professional help for emotional healing and working through underlying attachment issues.\n",
              "\n",
              "### Emotional Impact\n",
              "1.  Some people had a negative experience sharing very intimate details about their past with others, so this may cause distress.\n",
              "2.  The memories of the breakup can be quite vivid and powerful, as they are tied to strong emotions.\n",
              "\n",
              "### Common Themes\n",
              "1.  **Avoid emotional confrontation:** It sounds like the person fears confronting this conversation with the women from her past in order to avoid potential hurt or emotional discomfort.\n",
              "2.  **Potential for awkwardness:** Everyone involved from her past might view this as an invasion of personal space, and that there will indeed be awkward moments.\n",
              "\n",
              "### Specific Suggestions\n",
              "1.  **Emphasizing personal boundaries:** Encouraging others to respect the person's boundaries and maintain distance from the past.\n",
              "2.  **Considering the current relationship:** If the current boyfriend is understanding and supportive, the boundaries may be less of a challenge.\n",
              "\n",
              "**Discussion**\n",
              "\n",
              "\n",
              "\n",
              "*   \"If you're still in contact with these people, I think you should try to set boundaries around those interactions as well. You don't want to subject yourself to potentially awkward or uncomfortable situations without their consent.\"\n",
              "*   \"It's possible that these people view you as a close friend or a confidant, rather than as someone with a past intimate relationship. You need to consider whether this is a healthy way to interact or not.\"\n",
              "*   \"Have you talked to your boyfriend about this? He may be more understanding than you think, and it's possible that he could help you navigate the situation or find a way to reduce the emotional discomfort.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat completion:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Your post is relatable, honest, and courageous. You're acknowledging the complexities of moving on from a toxic relationship and the emotions that still linger. Here are some thoughts and potential next steps based on your situation:\n",
              "\n",
              "1. **Recognize the value of your support network**: You mentioned that the girls from your past relationship provide emotional support. Having a strong support system can be beneficial during difficult times. However, it's essential to consider whether involving them in your decision to cut contact may be hurtful or confusing.\n",
              "2. **Potential impact on your current relationship**: Your thoughts about cutting contact with the girls may affect your current relationship. Depending on the timing and the relationship dynamics, they might see your actions as either understanding or defensive. Without proper communication, it's understandable that there could be awkwardness or hurt feelings.\n",
              "3. **Prioritize your emotional well-being**: It's crucial to remember that your past experiences have shaped your present. It might be helpful to acknowledge that the emotions you feel about the relationship with your ex-girlfriends are not a rejection of your current partner but rather a reflection of your personal growth and understanding of healthy relationships.\n",
              "4. **Consider having an open conversation with each girl privately**: Before making a public announcement, it might be beneficial to have a private, in-depth conversation with each of the girls you're considering cutting contact from. This will allow you to share your reasons, feelings, and experiences, giving each girl the opportunity to process their emotions and ask questions.\n",
              "\n",
              "To minimize awkwardness and potential unintended consequences, consider the following:\n",
              "\n",
              "- **Be empathetic and understanding**: Share your personal growth and the changes you've made since the breakup.\n",
              "- **Explain your reasoning**: Clearly articulate what you're trying to accomplish by cutting contact with the girls.\n",
              "- **Be prepared for a range of reactions**: Your ex-girlfriends might feel hurt, upset, or even defend their friendship by your current partner. Be patient and understanding if your emotions come up during these discussions.\n",
              "- **Set boundaries**: If you choose to cut contact with the girls, clarify that you're not seeking revenge or are committed to a healthier, more loving interaction with your current partner.\n",
              "\n",
              "If you decide to proceed, take the following steps:\n",
              "\n",
              "- **Choose the right time and place**: Schedule a private conversation when you're both relatively calm and not in a rush.\n",
              "- **Be honest and open**: Share your emotions, reasons, and experiences with each girl.\n",
              "- **Listen actively**: Allow each girl to share their feelings and concerns.\n",
              "- **Apologize if necessary**: If your words come across as hurtful or dismissive, acknowledge and apologize for your actions.\n",
              "\n",
              "Ultimately, prioritizing your emotional well-being and taking the time to establish a better understanding of yourself and your relationships is essential.\n",
              "\n",
              "One advice from Reddit might be to try out online therapy or seek support from a trusted friend or family member before making a decision. If you decide to go forward, setting boundaries and clarifying your intentions with each individual will be helpful."
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}