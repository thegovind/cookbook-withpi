{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/DSPy_Prompt_and_Few_Shots_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-masthead"
      },
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://play.withpi.ai/logo/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# DSPy Optimization\n",
        "\n",
        "This Colab is the companion to the DSPy Optimization Playground.  [DSPy](https://dspy.ai/) is a toolkit for optimizing an application's system prompt.  It establishes a baseline (by scoring a set of **Inputs** against a **Contract**), then experimenting with the application prompt to try and improve the contract scores over the inputs.\n",
        "\n",
        "See [Key Concepts](https://code.withpi.ai/key-concepts) if you want more details about Contracts.\n",
        "\n",
        "This Colab continues with the `Aesop AI` example and a test input set, but any will do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-setup-markdown"
      },
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pi-setup"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "from google.colab import files, userdata\n",
        "\n",
        "# Load the notebook secret into the environment so the Pi Client can access it.\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "%pip install withpi litellm httpx datasets jinja2 tqdm\n",
        "\n",
        "# Import a bunch of useful libraries for later.\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from collections import defaultdict\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import datasets\n",
        "import httpx\n",
        "import litellm\n",
        "import jinja2\n",
        "from tqdm.notebook import tqdm\n",
        "from withpi import PiClient\n",
        "from withpi.types import Contract\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "\n",
        "client = PiClient()\n",
        "\n",
        "\n",
        "def print_contract(contract: Contract):\n",
        "    \"\"\"print_contract pretty-prints a contract\"\"\"\n",
        "    for dimension in contract.dimensions:\n",
        "        print(dimension.label)\n",
        "        for sub_dimension in dimension.sub_dimensions:\n",
        "            print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "\n",
        "def generate(system: str, user: str, model: str) -> str:\n",
        "    \"\"\"generate passes the provided system and user prompts into the given model\n",
        "    via LiteLLM\"\"\"\n",
        "    messages = [\n",
        "        {\"content\": system, \"role\": \"system\"},\n",
        "        {\"content\": user, \"role\": \"user\"},\n",
        "    ]\n",
        "    return litellm.completion(model=model, messages=messages).choices[0].message.content\n",
        "\n",
        "\n",
        "class printer(str):\n",
        "    \"\"\"printer makes strings with embedded newlines print more nicely\"\"\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self\n",
        "\n",
        "\n",
        "def print_response(response: str):\n",
        "    \"\"\"print_response pretty-prints an LLM response, respecting newlines\"\"\"\n",
        "    display(printer(response))\n",
        "\n",
        "\n",
        "def print_scores(pi_scores):\n",
        "    \"\"\"print_scores pretty-prints a Pi Score response as a table.\"\"\"\n",
        "    for dimension_name, dimension_scores in pi_scores.dimension_scores.items():\n",
        "        print(f\"{dimension_name}: {dimension_scores.total_score}\")\n",
        "        for (\n",
        "            subdimension_name,\n",
        "            subdimension_score,\n",
        "        ) in dimension_scores.subdimension_scores.items():\n",
        "            print(f\"\\t{subdimension_name}: {subdimension_score}\")\n",
        "        print(\"\\n\")\n",
        "    print(\"---------------------\")\n",
        "    print(f\"Total score: {pi_scores.total_score}\")\n",
        "\n",
        "\n",
        "def save_file(filename: str, model: str):\n",
        "    \"\"\"save_file offers to download the model with the given filename\"\"\"\n",
        "    Path(filename).write_text(model)\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "def load_contract(url: str) -> Contract:\n",
        "    \"\"\"load_contract pulls a Contract JSON blob locally with validation.\"\"\"\n",
        "    resp = httpx.get(url)\n",
        "    return Contract.model_validate_json(resp.content)\n",
        "\n",
        "\n",
        "def load_and_split_dataset(url: str) -> datasets.DatasetDict:\n",
        "    \"\"\"load_and_split_dataset pulls in the Parquet file at url and does a 90/10 split\"\"\"\n",
        "    return datasets.load_dataset(\n",
        "        \"parquet\", data_files=url, split=\"train\"\n",
        "    ).train_test_split(test_size=0.1)\n",
        "\n",
        "\n",
        "def do_bulk_inference(dataset, system, model):\n",
        "    \"\"\"do_bulk_inference performs inference on the 'input' column of dataset, using\n",
        "    the provided system prompt.  The model identified will be used via LiteLLM\"\"\"\n",
        "\n",
        "    def do_generate(user, pbar):\n",
        "        result = generate(system, user, model)\n",
        "        pbar.update(1)\n",
        "        return result\n",
        "\n",
        "    futures = []\n",
        "    pbar = tqdm(total=len(dataset))\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for row in dataset:\n",
        "            futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "    return [future.result() for future in futures]\n",
        "\n",
        "\n",
        "def do_bulk_templated_inference(dataset, optimized, model):\n",
        "    \"\"\"do_bulk_templated_inference performs inference on the 'input' column of dataset,\n",
        "    using the provided optimized prompt.  It should be a Jinja2 template as returned\n",
        "    by DSPy\"\"\"\n",
        "    prompt_template = jinja2.Template(optimized)\n",
        "    result_extractor = re.compile(\n",
        "        r\".*\\[\\[ ## response ## \\]\\](.*)\\[\\[ ## completed ## \\]\\]\", re.DOTALL\n",
        "    )\n",
        "\n",
        "    def do_generate(prompt: str, pbar) -> str:\n",
        "        messages = json.loads(prompt_template.render(input=prompt))\n",
        "        result = (\n",
        "            litellm.completion(model=model, messages=messages)\n",
        "            .choices[0]\n",
        "            .message.content\n",
        "        )\n",
        "\n",
        "        pbar.update(1)\n",
        "        return result_extractor.match(result).group(1)\n",
        "\n",
        "    futures = []\n",
        "    pbar = tqdm(total=len(dataset))\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for row in dataset:\n",
        "            futures.append(executor.submit(do_generate, row[\"input\"], pbar))\n",
        "    return [future.result() for future in futures]\n",
        "\n",
        "\n",
        "def generate_table(\n",
        "    job_id: str, training_data: dict, is_done: bool, additional_columns: dict[str, str]\n",
        "):\n",
        "    \"\"\"Generate a training progress table dynamically.\"\"\"\n",
        "    data_dict = {}\n",
        "    for header in [\"Step\", \"Epoch\", \"Learning Rate\", \"Training Loss\", \"Eval Loss\"]:\n",
        "        data_dict[header] = []\n",
        "    for header in additional_columns.keys():\n",
        "        data_dict[header] = []\n",
        "\n",
        "    for step, data in training_data.items():\n",
        "        data_dict[\"Step\"].append(step)\n",
        "        for header, key in [\n",
        "            (\"Epoch\", \"epoch\"),\n",
        "            (\"Learning Rate\", \"learning_rate\"),\n",
        "            (\"Training Loss\", \"loss\"),\n",
        "            (\"Eval Loss\", \"eval_loss\"),\n",
        "        ]:\n",
        "            data_dict[header].append(data.get(key, \"X\"))\n",
        "        for header, key in additional_columns.items():\n",
        "            data_dict[header].append(data.get(key, \"X\"))\n",
        "\n",
        "    if not is_done:\n",
        "        data_dict[\"Step\"].append(\"...\")\n",
        "        for header in [\"Epoch\", \"Learning Rate\", \"Training Loss\", \"Eval Loss\"]:\n",
        "            data_dict[header].append(\"\")\n",
        "        for header in additional_columns.keys():\n",
        "            data_dict[header].append(\"\")\n",
        "\n",
        "    return pd.DataFrame(data_dict)\n",
        "\n",
        "\n",
        "def stream_response(job_id: str, method, additional_columns: dict[str, str]):\n",
        "    \"\"\"stream_response streams messages from the provided method\n",
        "\n",
        "    method should be a Pi client object with `retrieve` and `stream_messages`\n",
        "    endpoints.  This is primarily for convenience.\"\"\"\n",
        "\n",
        "    print(f\"Training Status for {job_id}\")\n",
        "\n",
        "    training_data = defaultdict(dict)\n",
        "    is_log_console = False\n",
        "\n",
        "    stream_output = display(\n",
        "        generate_table(\n",
        "            job_id, training_data, is_done=False, additional_columns=additional_columns\n",
        "        ),\n",
        "        display_id=True,\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        response = method.retrieve(job_id=job_id)\n",
        "        if (response.state != \"QUEUED\") and (response.state != \"RUNNING\"):\n",
        "            if response.state == \"DONE\" and not is_log_console:\n",
        "                for line in response.detailed_status:\n",
        "                    try:\n",
        "                        data_dict = json.loads(line)\n",
        "                        training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                stream_output.update(\n",
        "                    generate_table(\n",
        "                        job_id,\n",
        "                        training_data,\n",
        "                        is_done=True,\n",
        "                        additional_columns=additional_columns,\n",
        "                    )\n",
        "                )\n",
        "            return response\n",
        "\n",
        "        with method.with_streaming_response.stream_messages(\n",
        "            job_id=job_id, timeout=None\n",
        "        ) as response:\n",
        "            is_done = False\n",
        "            for line in response.iter_lines():\n",
        "                if line == \"DONE\":\n",
        "                    is_done = True\n",
        "                try:\n",
        "                    data_dict = json.loads(line)\n",
        "                    training_data[data_dict[\"step\"]].update(data_dict)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                stream_output.update(\n",
        "                    generate_table(\n",
        "                        job_id,\n",
        "                        training_data,\n",
        "                        is_done,\n",
        "                        additional_columns=additional_columns,\n",
        "                    )\n",
        "                )\n",
        "                is_log_console = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load contract and Dataset\n",
        "\n",
        "Load the `Aesop AI` example and example set from Pi Labs cookbooks, or edit below to load a different one.\n",
        "\n",
        "This is using Hugging Face datasets, so any published dataset you can access should work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "aesop_contract = load_contract(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai.json\")\n",
        "aesop = load_and_split_dataset(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/datasets/aesop_ai_examples.parquet\")\n",
        "aesop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Optimize your prompt\n",
        "\n",
        "Kick off a prompt optimization run.  This will operate in the background and will take order of **10 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4NpCZhP6exgI"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_status = client.prompt.optimize(\n",
        "    contract=aesop_contract,\n",
        "    initial_system_instruction=aesop_contract.description,\n",
        "    examples=[{\"llm_input\": row[\"input\"], \"llm_output\": row[\"output\"]} for row in aesop['train']],\n",
        "    model_id=\"gpt-4o-mini\",\n",
        "    tuning_algorithm=\"DSPY\",\n",
        "    dspy_optimization_type=\"COPRO\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stream the messages as the inputs are generated\n",
        "\n",
        "The messages provide detail about what is being done as the prompt is getting optimized."
      ],
      "metadata": {
        "id": "GA3kTgGaQquj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    prompt_optimization_status = client.prompt.retrieve(prompt_optimization_status.job_id)\n",
        "    if prompt_optimization_status.state in [\"ERROR\", \"DONE\"]:\n",
        "        break\n",
        "\n",
        "    # Steam Messages\n",
        "    with client.prompt.with_streaming_response.stream_messages(prompt_optimization_status.job_id) as response:\n",
        "        for line in response.iter_lines():\n",
        "            print(line)"
      ],
      "metadata": {
        "id": "Z6umJtKEPjbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYT2a6-7zHVf"
      },
      "source": [
        "## Check out the optimized prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01LXvjOMte7D"
      },
      "outputs": [],
      "source": [
        "optimized_prompt = json.dumps(prompt_optimization_status.optimized_prompt_messages, indent=2)\n",
        "print(optimized_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMTmieU9xPgT"
      },
      "source": [
        "## Save the new system prompt template\n",
        "\n",
        "It's convenient to stash this template for use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tpWpFWfEwo4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ece4dcb4-df6d-409c-9cc5-d52b950edc0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_461df38b-324e-4fd0-abe3-b65f2f6c1dbf\", \"aesop_ai_dspy_prompt.json.jinja\", 1119)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "save_file('aesop_ai_dspy_prompt.json.jinja', optimized_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsAFbTlxctKF"
      },
      "source": [
        "## (if resuming) Load system prompt\n",
        "\n",
        "If you don't want to wait, load the pre-optimized one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sXlZ2hPRcs4e"
      },
      "outputs": [],
      "source": [
        "optimized = httpx.get(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/prompts/aesop_ai_dspy_prompt.json.jinja\").text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXdTY-z8zpn7"
      },
      "source": [
        "#Run inference with the test split\n",
        "\n",
        "DSPy emits a Jinja2-style template, so inference requires some template substitution.  Let's compare performance on the holdout set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZk4WJN80Ikc"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "aesop_updated = aesop['test'].add_column('original_output', do_bulk_inference(aesop['test'], aesop_contract.description, \"gemini/gemini-1.5-flash-8b\")).add_column('optimized_output', do_bulk_templated_inference(aesop['test'], optimized, \"gemini/gemini-1.5-flash-8b\"))\n",
        "aesop_updated\n",
        "\n",
        "original_scores = [client.contracts.score(\n",
        "    contract=aesop_contract,\n",
        "    llm_input=row[\"input\"],\n",
        "    llm_output=row[\"original_output\"],\n",
        ").total_score for row in aesop_updated]\n",
        "\n",
        "print(\"Original Scores\")\n",
        "print(original_scores)\n",
        "\n",
        "optimized_scores = [client.contracts.score(\n",
        "    contract=aesop_contract,\n",
        "    llm_input=row[\"input\"],\n",
        "    llm_output=row[\"optimized_output\"],\n",
        ").total_score for row in aesop_updated]\n",
        "\n",
        "print(\"Optimized Scores\")\n",
        "print(optimized_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing Few Shots\n",
        "\n",
        "Details of the few shots optimization are similar. We use DSPy's `BOOTSTRAP_FEW_SHOT` algorithm here.\n",
        "\n",
        "We kick off a job first, then read the messages as the few shots are getting optimized and finally print the prompt with optimized few shots in it."
      ],
      "metadata": {
        "id": "5JZOCnJmUvFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shot_optimization_status = client.prompt.optimize(\n",
        "    contract=aesop_contract,\n",
        "    initial_system_instruction=aesop_contract.description,\n",
        "    examples=[{\"llm_input\": row[\"input\"], \"llm_output\": row[\"output\"]} for row in aesop['train']],\n",
        "    model_id=\"gpt-4o-mini\",\n",
        "    tuning_algorithm=\"DSPY\",\n",
        "    dspy_optimization_type=\"BOOTSTRAP_FEW_SHOT\",\n",
        "    use_chain_of_thought=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Jsvwh8edU0hP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    shot_optimization_status = client.prompt.retrieve(shot_optimization_status.job_id)\n",
        "    if shot_optimization_status.state in [\"ERROR\", \"DONE\"]:\n",
        "        break\n",
        "\n",
        "    # Steam Messages\n",
        "    with client.prompt.with_streaming_response.stream_messages(shot_optimization_status.job_id) as response:\n",
        "        for line in response.iter_lines():\n",
        "            print(line)"
      ],
      "metadata": {
        "id": "EIQTNtO7Vp1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_shots = json.dumps(shot_optimization_status.optimized_prompt_messages, indent=2)\n",
        "print(optimized_shots)\n"
      ],
      "metadata": {
        "id": "2B_kCjO3WRs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7s_nuJsHbM"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Now you have an improved prompt on a small sample set.  You could deploy this now, but improving the training set or the contract will give you better performance.  Check out the rest of the playgrounds to proceed from here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}