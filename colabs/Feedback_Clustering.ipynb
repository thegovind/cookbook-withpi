{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Feedback_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logoFullBlack.svg\" width=\"240\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://play.withpi.ai\"><font size=\"4\">Technique Catalog</font></a>"
      ],
      "metadata": {
        "id": "pi-masthead"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwm4tjdnedp6"
      },
      "source": [
        "# WithPi Feedback Clustering\n",
        "\n",
        "This colab assumes that you have a contract and want to see how clutering works on Feedback.\n",
        "\n",
        "We will walk through the same `Aesop AI` example, but you can load any contract here.  Ideally you would have some real feedback, but for this Colab we'll demonstrate with synthetic feedback.\n",
        "\n",
        "This should take about **15 minutes**, even if you're unfamiliar with Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and initialize SDK\n",
        "\n",
        "Connect to a regular CPU Python 3 runtime.  You won't need GPUs for this notebook.\n",
        "\n",
        "You'll need a WITHPI_API_KEY from https://play.withpi.ai.  Add it to your notebook secrets (the key symbol) on the left.\n",
        "\n",
        "Run the cell below to install packages and load the SDK"
      ],
      "metadata": {
        "id": "pi-setup-markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi-setup"
      },
      "source": [
        "%%capture\n",
        "\n",
        "%pip install withpi litellm\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from withpi import PiClient\n",
        "\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n",
        "\n",
        "client = PiClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RRO3iXjYbY"
      },
      "source": [
        "# Load contract and Dataset\n",
        "\n",
        "Load the `Aesop AI` example and example set from Pi Labs cookbooks, or edit below to load a different one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJmb89i5iN5"
      },
      "outputs": [],
      "source": [
        "import httpx\n",
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "from withpi.types import Contract\n",
        "\n",
        "resp = httpx.get(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/contracts/aesop_ai_calibrated.json\")\n",
        "\n",
        "aesop_contract = Contract.model_validate_json(resp.content)\n",
        "\n",
        "for dimension in aesop_contract.dimensions:\n",
        "  print(dimension.label)\n",
        "  for sub_dimension in dimension.sub_dimensions:\n",
        "    print(f\"\\t{sub_dimension.description}\")\n",
        "\n",
        "df = pd.read_parquet(\"https://raw.githubusercontent.com/withpi/cookbook-withpi/refs/heads/main/datasets/aesop_ai_examples.parquet\")\n",
        "data_table.enable_dataframe_formatter()\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FAoBqU7dwf"
      },
      "source": [
        "## Generate Feedback\n",
        "\n",
        "In the real world, you would have feedback from either internal to your team, your users, or some other source.  We will generate feedback from a fairly capable LLM instead, though this method will be unlikely to give you useful examples to learn from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxVGkJTtmSmy"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm.notebook import tqdm\n",
        "from litellm import completion\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "def generate(llm_input: str, llm_output: str, pbar) -> str:\n",
        "  messages = [\n",
        "      {\n",
        "          \"content\": f\"You will get an input (wrapped in <input></input> tags) and an output (wrapped in <output></output> tags) produced by an LLM trying to implement an application described by <application>{aesop_contract.description}</application>. Generate one line of feedback as if you were a user rating this output. Do not include any tags, just the feedback.\",\n",
        "          \"role\": \"system\"\n",
        "      },\n",
        "      {\n",
        "          \"content\": f\"<input>{llm_input}</input><output>{llm_output}</output>\",\n",
        "          \"role\": \"user\"\n",
        "      },\n",
        "  ]\n",
        "  result = completion(model=\"gemini/gemini-1.5-flash-8b-latest\",\n",
        "                      messages=messages).choices[0].message.content\n",
        "  pbar.update(1)\n",
        "  return result\n",
        "\n",
        "def do_inference():\n",
        "  futures = []\n",
        "  pbar = tqdm(total=len(df))\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for index, row in df.iterrows():\n",
        "      futures.append(executor.submit(generate, row[\"input\"], row[\"output\"], pbar))\n",
        "  return [future.result() for future in futures]\n",
        "\n",
        "df[\"feedback\"] = do_inference()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjwpaF5yoh19"
      },
      "source": [
        "## Cluster feedback\n",
        "\n",
        "Now try clustering the feedback provided.\n",
        "\n",
        "We only generated textual feedbacks, but if the user had a way to rate a response, that can be included as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NpCZhP6exgI"
      },
      "outputs": [],
      "source": [
        "feedback_clusters = client.feedback.cluster(\n",
        "    feedbacks=[\n",
        "        {\"identifier\": str(index),\n",
        "         \"llm_input\": row[\"input\"],\n",
        "         \"llm_output\": row[\"output\"],\n",
        "         \"comment\": row[\"feedback\"],\n",
        "         \"sources\": [\"Synthetic\"], # Can include labels from different sources.\n",
        "         \"rating\": \"Neutral\" # Clustering can include the rating if available.\n",
        "         } for index, row in df.iterrows()],\n",
        ")\n",
        "\n",
        "for cluster in feedback_clusters:\n",
        "  display(f\"Cluster name: {cluster.topic}, Count: {len(cluster.feedback)}\")\n",
        "\n",
        "df['cluster'] = ['']*len(df)\n",
        "for cluster in feedback_clusters:\n",
        "  for identifier in cluster.feedback:\n",
        "    df.loc[int(identifier),'cluster'] = cluster.topic\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCHE6g_OCqCD"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Now it may be useful to slice and dice this data to identify areas for improvement.  If a lot of users are saying similar things, make sure there is a Dimension that focuses on it.  This will penalize responses that fail that attribute, meaning you can return to [Prompt Optimization](https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Prompt_Optimization.ipynb) to automatically incorporate this feedback.  Coming soon you'll be able to deploy a model with reinforcement learning that is steered in this direction.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}