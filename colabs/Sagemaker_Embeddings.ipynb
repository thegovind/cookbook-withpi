{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withpi/cookbook-withpi/blob/main/colabs/Sagemaker_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://withpi.ai\"><img src=\"https://withpi.ai/logo/logoFullBlack.svg\" width=\"240px\"></a>\n",
        "\n",
        "<a href=\"https://code.withpi.ai\"><font size=\"4\">Documentation</font></a>\n",
        "\n",
        "<a href=\"https://withpi.ai\"><font size=\"4\">Copilot</font></a>"
      ],
      "metadata": {
        "id": "Th0kWr38rG1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "\n",
        "Pi has published an API for performing embedding inference.\n",
        "\n",
        "It takes as input a list of items to embed and returns a list of embeddings.\n",
        "\n",
        "It can be deployed to Sagemaker for inference in your own account.  This notebook shows how to perform inference with it."
      ],
      "metadata": {
        "id": "S7uqCaRarJg7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbxkKAn8Us-1",
        "outputId": "ab929007-b3e3-4235-ca3e-942e02943f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install httpx tqdm\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"WITHPI_API_KEY\"] = userdata.get('WITHPI_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import httpx\n",
        "import time\n",
        "\n",
        "async with httpx.AsyncClient() as client:\n",
        "  latencies = []\n",
        "  for _ in range(10):\n",
        "    start = time.perf_counter()\n",
        "    resp = await client.post(\n",
        "      \"https://api.withpi.ai/v1/search/embed\",\n",
        "      headers={\n",
        "        \"x-api-key\": os.environ['WITHPI_API_KEY'],\n",
        "      },\n",
        "      json={\n",
        "        \"query\": [\"Some document to embed\"],\n",
        "        \"batch\": False,\n",
        "      },\n",
        "      timeout=10.0\n",
        "    )\n",
        "    stop = time.perf_counter()\n",
        "    latencies.append(f\"{stop-start:.3f}\")\n",
        "\n",
        "  resp.raise_for_status()\n",
        "  print(f\"Tokens processed: {resp.headers['x-tokens-processed']}\")\n",
        "  print(f\"Latencies: {latencies}\")\n",
        "  parsed = resp.json()\n",
        "  print(f\"Number of embeddings: {len(parsed)}\")\n",
        "  print(f\"Dimensionality of embeddings: {len(parsed[0])}\")\n",
        "  print(f\"Sample: {parsed[0][:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJskVOdtvV1-",
        "outputId": "645f9d91-4dc8-4bbb-f7f8-28dce7f55f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens processed: 6\n",
            "Latencies: ['0.736', '0.382', '0.143', '0.142', '0.136', '0.479', '0.137', '0.146', '0.140', '0.143']\n",
            "Number of embeddings: 1\n",
            "Dimensionality of embeddings: 256\n",
            "Sample: [-0.08392333984375, -0.072998046875, -0.00856781005859375, -0.039306640625, 0.08184814453125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calling at scale\n",
        "\n",
        "Any number of documents can be sent in one request.  The backend maintains\n",
        "a queue and will dynamically group small requests into larger batches for throughput.  It will also slice overly large requests into pieces for execution.\n",
        "\n",
        "The \"batch\" parameter controls whether to hit an ONNX model tuned to a batch size of 1 (for minimal online latency) or with a Torch model configured to a batch size of 32 (to maximize throughput).  The input list can be any length, but picking one of those two sizes will maximize throughput.\n",
        "\n",
        "We recommend limiting concurrency as a throttling mechanism, since that will avoid runaway queue growth.  See below for an example that sends 50k documents quickly.\n",
        "\n"
      ],
      "metadata": {
        "id": "dH4zM0JBw_do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from itertools import islice\n",
        "import httpx\n",
        "import random\n",
        "import time\n",
        "import tqdm.asyncio\n",
        "\n",
        "class Counter:\n",
        "  def __init__(self):\n",
        "    self.token_count = 0\n",
        "  def increment(self, amount):\n",
        "    self.token_count += amount\n",
        "\n",
        "def make_fake_documents():\n",
        "  docs = []\n",
        "  for _ in range(50000):\n",
        "    docs.append(\" \".join([\"word\"]*random.randint(512, 512*3)))\n",
        "  return docs\n",
        "\n",
        "def batched(iterable, n):\n",
        "    \"Batch data into tuples of length n. The last batch may be shorter.\"\n",
        "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
        "    if n < 1:\n",
        "        raise ValueError('n must be at least one')\n",
        "    it = iter(iterable)\n",
        "    while batch := tuple(islice(it, n)):\n",
        "        yield batch\n",
        "\n",
        "corpus = make_fake_documents()\n",
        "concurrency_limit = asyncio.Semaphore(64)\n",
        "token_counter = Counter()\n",
        "\n",
        "async with httpx.AsyncClient() as client:\n",
        "  async def call_batch(batch):\n",
        "    async with concurrency_limit:\n",
        "      attempts = 0\n",
        "      while attempts < 3:\n",
        "        attempts += 1\n",
        "        resp = await client.post(\n",
        "          \"https://api.withpi.ai/v1/search/embed\",\n",
        "          headers={\n",
        "            \"x-api-key\": os.environ['WITHPI_API_KEY'],\n",
        "          },\n",
        "          json={\n",
        "            \"query\": batch,\n",
        "            \"batch\": True,\n",
        "          },\n",
        "          timeout=10.0\n",
        "        )\n",
        "        if resp.status_code == 200:\n",
        "          # No lock because this is single-threaded asyncio.\n",
        "          # Othewise protect this.\n",
        "          token_counter.increment(int(resp.headers['x-tokens-processed']))\n",
        "          return\n",
        "        # Sleep while holding the concurrency limiter to force a slowdown.\n",
        "        print(f\"Status code: {resp.status_code}\")\n",
        "        print(f\"Retrying after {attempts} seconds\")\n",
        "        await asyncio.sleep(attempts)\n",
        "      raise ValueError(\"Retries exhausted...shutting down\")\n",
        "\n",
        "  start = time.perf_counter()\n",
        "  try:\n",
        "    async with asyncio.TaskGroup() as tg:\n",
        "      tasks = []\n",
        "      for batch in batched(corpus, 32):\n",
        "        tasks.append(tg.create_task(call_batch(batch)))\n",
        "      for f in tqdm.asyncio.tqdm.as_completed(tasks):\n",
        "        await f\n",
        "  except ExceptionGroup as e:\n",
        "    print(e.exceptions)\n",
        "    raise\n",
        "\n",
        "\n",
        "  stop = time.perf_counter()\n",
        "  elapsed = stop - start\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(f\"Total tokens processed: {token_counter.token_count}\")\n",
        "  print(f\"Elapsed time: {elapsed:.2f} seconds\")\n",
        "  print(f\"Throughput: {token_counter.token_count / elapsed:.2f} tokens/second\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4MpgjBLxlWc",
        "outputId": "ff3b9d82-598b-49ea-b149-97e0b244905f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:55<00:00, 28.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Total tokens processed: 51250480\n",
            "Elapsed time: 55.78 seconds\n",
            "Throughput: 918782.43 tokens/second\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}