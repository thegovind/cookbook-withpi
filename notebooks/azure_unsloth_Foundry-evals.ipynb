{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1803db",
   "metadata": {},
   "source": [
    "# Low-VRAM PEFT (LoRA) fine-tuning with Pi-Scorer on a small Unsloth model\n",
    "\n",
    "Welcome! This notebook shows how to fine-tune a small open model with **LoRA/PEFT** on a tiny, realistic dataset,\n",
    "evaluate with **Pi-Scorer** (fast, deterministic scorers), and optionally upload an evaluation set to **Azure AI Foundry**.\n",
    "\n",
    "You will:\n",
    "1. Install compatible libraries.\n",
    "2. Load a 4-bit Unsloth model and attach LoRA adapters (low VRAM).\n",
    "3. Create a small **customer-support** dataset.\n",
    "4. (Bootstrap) Generate teacher replies once to create supervision labels for SFT.\n",
    "5. Run **SFT training** with LoRA.\n",
    "6. Save the LoRA adapter.\n",
    "7. Generate answers and score them with **Pi-Scorer**.\n",
    "8. (Optional) Upload a JSONL to **Azure AI Foundry** and launch a cloud evaluation.\n",
    "\n",
    "### Hardware notes\n",
    "- A single 16–24 GB GPU (A10, L4) is enough.\n",
    "- On **ND H100 v5** (H100 Tensor Core) in Azure, `bf16` is used automatically where possible.\n",
    "\n",
    "### Why SFT + LoRA first?\n",
    "Reinforcement learning (GRPO) can be sensitive to shapes and trainer internals. A **LoRA SFT baseline** is simpler,\n",
    "stable, and a great starting point. You can bring Pi-Scorer back as a reward for GRPO later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ad13",
   "metadata": {},
   "source": [
    "## 0) Install required packages\n",
    "\n",
    "This cell pins versions that play nicely together for LoRA SFT with Unsloth. It may take a few minutes on a fresh runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core stack (Unsloth + TRL + Transformers + vLLM)\n",
    "!pip -q install unsloth vllm==0.8.5.post1\n",
    "!pip -q install bitsandbytes accelerate xformers peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
    "!pip -q install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "!pip -q install transformers==4.51.3 ipywidgets requests\n",
    "\n",
    "# Azure AI Foundry (Projects SDK) to upload dataset & run cloud evaluations\n",
    "!pip -q install azure-ai-projects azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d009701",
   "metadata": {},
   "source": [
    "## 1) Configure keys (Pi-Scorer required, Azure optional)\n",
    "\n",
    "This cell *only* reads environment variables (and prompts if missing). Nothing is sent anywhere yet.\n",
    "\n",
    "- **WITHPI_API_KEY** is required (https://build.withpi.ai/account/keys)\n",
    "- For Azure evaluation later, you may set either:\n",
    "  - `AZURE_AI_PROJECT` (preferred) — e.g. `https://<account>.services.ai.azure.com/api/projects/<project>` and rely on `DefaultAzureCredential()`\n",
    "  - or an older `AZURE_AI_PROJECT_CONNECTION_STRING` if your SDK environment still uses it.\n",
    "- If you will use LLM-judged built-ins, set `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, and `AZURE_OPENAI_DEPLOYMENT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e2048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHPI_API_KEY set? True\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _need(key: str, prompt: str) -> str:\n",
    "    val = os.environ.get(key)\n",
    "    if val:\n",
    "        return val\n",
    "    try:\n",
    "        val = getpass.getpass(prompt)\n",
    "    except Exception:\n",
    "        val = input(prompt)\n",
    "    if val:\n",
    "        os.environ[key] = val.strip()\n",
    "    return os.environ.get(key, \"\")\n",
    "\n",
    "# Required for Pi-Scorer reward calls\n",
    "_need(\"WITHPI_API_KEY\", \"Enter your WITHPI_API_KEY (input hidden): \")\n",
    "print(\"WITHPI_API_KEY set?\", bool(os.environ.get(\"WITHPI_API_KEY\")))\n",
    "\n",
    "# Optional for Azure evaluation later (uncomment and fill if you prefer setting here)\n",
    "# os.environ.setdefault(\"AZURE_AI_PROJECT\", \"https://<account>.services.ai.azure.com/api/projects/<project>\")\n",
    "# os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\", \"https://<your-aoai>.openai.azure.com/\")\n",
    "# os.environ.setdefault(\"AZURE_OPENAI_API_KEY\", \"<key>\")\n",
    "# os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\", \"<gpt-4o-mini / o4-mini / ...>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1aa6c",
   "metadata": {},
   "source": [
    "## 2) Load a small Unsloth model and attach LoRA (PEFT)\n",
    "\n",
    "We use **`unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit`** by default. If you are extremely VRAM-constrained, change `model_name` to **`unsloth/Qwen2.5-0.5B-Instruct-unsloth-bnb-4bit`**.\n",
    "\n",
    "- `load_in_4bit=True` keeps the base model small.\n",
    "- `get_peft_model(...)` attaches LoRA adapters so we train only a tiny fraction of weights.\n",
    "- We also set a few tokenizer/model flags for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7e30b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] Trying: fast_inference=True, trust_remote_code=False\n",
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.8.9: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA H100 NVL. Num GPUs = 1. Max memory: 93.016 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-1.5b-instruct-bnb-4bit with actual GPU utilization = 43.94%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 93.02 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 39.54 GB. Also swap space = 6 GB.\n",
      "INFO 08-27 01:32:50 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 08-27 01:32:50 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=1024.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cc94e4afae4c79a5e52028f9e679b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c25f9834d1049fba9b338cc722b2b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3a839c4073487dba9d370409875246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c5b43007e243c6814d0ec139be4414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76519e571245472b8ce85999c6b66683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d0f46d206c4f18b07aa2e6bc9eb39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f603a4135c4094aa70d12123a61f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-27 01:32:57 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen2.5-1.5b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-1.5b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-1.5b-instruct-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 08-27 01:32:57 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7379b0cd22a0>\n",
      "INFO 08-27 01:32:57 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-27 01:32:57 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 08-27 01:32:57 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-27 01:32:57 [gpu_model_runner.py:1329] Starting to load model unsloth/qwen2.5-1.5b-instruct-bnb-4bit...\n",
      "INFO 08-27 01:32:57 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 08-27 01:32:57 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a1a5313c76434d84a21492e2ac6b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-27 01:33:14 [weight_utils.py:281] Time spent downloading weights for unsloth/qwen2.5-1.5b-instruct-bnb-4bit: 16.076469 seconds\n",
      "INFO 08-27 01:33:14 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735f1acc468f429094da3f09a509471a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fd749240da4098ae8d889a47227d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-27 01:33:15 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 08-27 01:33:15 [gpu_model_runner.py:1347] Model loading took 1.2132 GiB and 18.115439 seconds\n",
      "INFO 08-27 01:33:26 [backends.py:420] Using cache directory: /home/govind/.cache/vllm/torch_compile_cache/7061a7a5b6/rank_0_0 for vLLM's torch.compile\n",
      "INFO 08-27 01:33:26 [backends.py:430] Dynamo bytecode transform time: 10.32 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:01<00:00,  3.66it/s, triton_poi_fused_cat_4]                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-27 01:33:32 [backends.py:136] Cache the graph of shape None for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 23.19it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 147.60it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 148.33it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 150.96it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 65.29it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 144.39it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 146.83it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 74.18it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 96.38it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 79.34it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 64.87it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 79.76it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 77.20it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 71.16it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 76.54it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 77.51it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 92.16it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 145.48it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 146.15it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 144.54it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 145.59it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 144.54it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 145.59it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 68.08it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 144.15it/s, triton_poi_fused_cat_8]                            \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 93.50it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 9/9 [00:00<00:00, 81.09it/s, triton_poi_fused_cat_8]                             \n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 21.45it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-27 01:33:57 [backends.py:148] Compiling a graph for general shape takes 29.47 s\n",
      "INFO 08-27 01:34:20 [monitor.py:33] torch.compile takes 39.79 s in total\n",
      "INFO 08-27 01:34:21 [kv_cache_utils.py:634] GPU KV cache size: 990,464 tokens\n",
      "INFO 08-27 01:34:21 [kv_cache_utils.py:637] Maximum concurrency for 1,024 tokens per request: 967.25x\n",
      "INFO 08-27 01:34:21 [vllm_utils.py:643] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "INFO 08-27 01:34:21 [vllm_utils.py:643] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "INFO 08-27 01:34:39 [gpu_model_runner.py:1686] Graph capturing finished in 18 secs, took 6.96 GiB\n",
      "INFO 08-27 01:34:39 [vllm_utils.py:650] Unsloth: Patched vLLM v1 graph capture finished in 18 secs.\n",
      "INFO 08-27 01:34:39 [vllm_utils.py:650] Unsloth: Patched vLLM v1 graph capture finished in 18 secs.\n",
      "INFO 08-27 01:34:40 [core.py:159] init engine (profile, create kv cache, warmup model) took 84.91 seconds\n",
      "Unsloth: Just some info: will skip parsing ['k_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'q_norm']\n",
      "Unsloth: Just some info: will skip parsing ['k_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'q_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit | flags: {'fast_inference': True, 'trust_remote_code': False}\n",
      "Device: NVIDIA H100 NVL | bfloat16: True\n"
     ]
    }
   ],
   "source": [
    "# --- Unsloth PEFT load with automatic compatibility for trust_remote_code vs fast_inference ---\n",
    "from unsloth import FastLanguageModel\n",
    "import os, torch\n",
    "\n",
    "# Caches (same as before)\n",
    "os.environ.setdefault(\"HF_HOME\", \"/mnt/hf/cache\")\n",
    "os.environ.setdefault(\"HF_HUB_CACHE\", \"/mnt/hf/cache/hub\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", \"/mnt/hf/cache/hub\")\n",
    "\n",
    "max_seq_length = 1024\n",
    "lora_rank = 64\n",
    "\n",
    "# Pick one of these small, PEFT-friendly models:\n",
    "model_name = \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\"       # ~1.5B, great on a single 16–24GB GPU\n",
    "# model_name = \"unsloth/Qwen2.5-0.5B-Instruct-unsloth-bnb-4bit\"  # ~0.5B, ultra small\n",
    "\n",
    "def _load_unsloth(model_name: str):\n",
    "    # 1) Try fast path (fast_inference=True) with trust_remote_code=False\n",
    "    try:\n",
    "        print(\"[load] Trying: fast_inference=True, trust_remote_code=False\")\n",
    "        m, t = FastLanguageModel.from_pretrained(\n",
    "            model_name           = model_name,\n",
    "            max_seq_length       = max_seq_length,\n",
    "            load_in_4bit         = True,\n",
    "            fast_inference       = True,          # fast path (vLLM-style kernels)\n",
    "            max_lora_rank        = lora_rank,\n",
    "            gpu_memory_utilization = 0.5,\n",
    "            trust_remote_code    = False,         # <-- important: avoid the NotImplementedError\n",
    "            cache_dir            = os.environ.get(\"HF_HUB_CACHE\", None),\n",
    "        )\n",
    "        return m, t, dict(fast_inference=True, trust_remote_code=False)\n",
    "    except NotImplementedError as e:\n",
    "        # 2) Fallback: if the repo actually needs remote code, drop fast_inference\n",
    "        print(\"[load] Fast path + trust_remote_code not compatible; falling back \"\n",
    "              \"to fast_inference=False, trust_remote_code=True\")\n",
    "        m, t = FastLanguageModel.from_pretrained(\n",
    "            model_name           = model_name,\n",
    "            max_seq_length       = max_seq_length,\n",
    "            load_in_4bit         = True,\n",
    "            fast_inference       = False,         # <-- compatible with remote code\n",
    "            max_lora_rank        = lora_rank,\n",
    "            gpu_memory_utilization = 0.5,\n",
    "            trust_remote_code    = True,\n",
    "            cache_dir            = os.environ.get(\"HF_HUB_CACHE\", None),\n",
    "        )\n",
    "        return m, t, dict(fast_inference=False, trust_remote_code=True)\n",
    "\n",
    "model, tokenizer, _loader_flags = _load_unsloth(model_name)\n",
    "\n",
    "# Attach LoRA/PEFT\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# Tokenizer padding policy for your RL run\n",
    "tokenizer.padding_side = \"left\"      # GRPO prefers left-padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loaded:\", model_name, \"| flags:\", _loader_flags)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0), \"| bfloat16:\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6755805",
   "metadata": {},
   "source": [
    "## 3) Build your original **business support** dataset\n",
    "\n",
    "Exactly as before: the **system** message carries the policy, and the **user** message carries the ticket. For SFT, we also precompute a `text` field (a single string using the tokenizer’s chat template) that the trainer will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513d7989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b4af1d0ecc4b61b61abd28b0728002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a customer support agent for Contoso Retail. Write a short, professional email reply that:\\n- acknowledges the customer’s issue empathetically,\\n- provides clear next steps or resolution,\\n- follows the policy provided (do not contradict it),\\n- stays concise (≤150 words).\\n\\nPolicy:\\nDamaged on arrival: offer free replacement shipped via 2-day; if out of stock, offer full refund.', 'role': 'system'}, {'content': 'Order #78421 arrived with a cracked mug. Can you replace it? I need it before Friday.', 'role': 'user'}]\n",
      "--- sample text ---\n",
      "<|im_start|>system\n",
      "You are a customer support agent for Contoso Retail. Write a short, professional email reply that:\n",
      "- acknowledges the customer’s issue empathetically,\n",
      "- provides clear next steps or resolution,\n",
      "- follows the policy provided (do not contradict it),\n",
      "- stays concise (≤150 words).\n",
      "\n",
      "Policy:\n",
      "Damaged on arrival: offer free replacement shipped via 2-day; if out of stock, offer full refu ...\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a customer support agent for Contoso Retail. Write a short, professional email reply that:\\n\"\n",
    "    \"- acknowledges the customer’s issue empathetically,\\n\"\n",
    "    \"- provides clear next steps or resolution,\\n\"\n",
    "    \"- follows the policy provided (do not contradict it),\\n\"\n",
    "    \"- stays concise (≤150 words).\"\n",
    ")\n",
    "\n",
    "business_samples = [\n",
    "    {\n",
    "        \"ticket\": \"Order #78421 arrived with a cracked mug. Can you replace it? I need it before Friday.\",\n",
    "        \"policy\": \"Damaged on arrival: offer free replacement shipped via 2-day; if out of stock, offer full refund.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"I’m past the 30-day window but this shirt still has tags. Any chance I can return it?\",\n",
    "        \"policy\": \"Returns accepted within 30 days only; exceptions allowed as store credit at manager discretion.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"I canceled my order yesterday but I still see a pending charge on my card.\",\n",
    "        \"policy\": \"Cancellations void the authorization immediately; banks may take 3–5 business days to release funds.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"The promo code SPRING25 didn’t apply at checkout. Can you refund the difference?\",\n",
    "        \"policy\": \"SPRING25: 25% off full-price items only; cannot be combined; adjustments allowed within 7 days of purchase.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"I need to change the shipping address on my order to my office downtown.\",\n",
    "        \"policy\": \"Address changes allowed until fulfillment starts; otherwise reroute via carrier once tracking is issued.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"My gift card shows $0 after one use but I only spent $12 of $25.\",\n",
    "        \"policy\": \"Gift cards decrement in real-time; if balance mismatch occurs, reissue a new card with remaining funds.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"The espresso machine stopped working after two weeks. What can I do?\",\n",
    "        \"policy\": \"Appliances: 1-year warranty. Offer troubleshooting; if unresolved, advance replacement or repair.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"Do you price match Amazon on the headphones I bought yesterday?\",\n",
    "        \"policy\": \"Price match within 14 days against authorized retailers only; Amazon eligible when seller is Amazon.com.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(business_samples)\n",
    "\n",
    "def to_chat(ex):\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT + \"\\n\\nPolicy:\\n\" + ex[\"policy\"]},\n",
    "        {\"role\": \"user\", \"content\": ex[\"ticket\"]},\n",
    "    ]\n",
    "    return {\n",
    "        \"prompt\": msgs,\n",
    "        \"text\": tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(to_chat)\n",
    "print(dataset[0][\"prompt\"]) \n",
    "print(\"--- sample text ---\")\n",
    "print(dataset[0][\"text\"][:400] + \" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca8376",
   "metadata": {},
   "source": [
    "## 4) Pi-Scorer helpers (tone, resolution, policy)\n",
    "\n",
    "We reuse your three questions. These helpers will be used **after** SFT to score model replies on a handful of examples (quick, sanity-check style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b92835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "PI_API_KEY = os.environ[\"WITHPI_API_KEY\"]\n",
    "PI_API_URL = \"https://api.withpi.ai/v1/scoring_system/score\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"x-api-key\": PI_API_KEY}\n",
    "\n",
    "def get_pi_score(input_text: str, output_text: str, question: str) -> float:\n",
    "    payload = {\n",
    "        \"llm_input\": input_text,\n",
    "        \"llm_output\": output_text,\n",
    "        \"scoring_spec\": [{\"question\": question}]\n",
    "    }\n",
    "    resp = requests.post(PI_API_URL, headers=HEADERS, json=payload, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if \"total_score\" not in data:\n",
    "        raise KeyError(\"'total_score' missing in Pi response\")\n",
    "    return float(data[\"total_score\"])\n",
    "\n",
    "def _compose_business_input_from_prompt(prompt_messages: list[dict]) -> str:\n",
    "    system_text = prompt_messages[0][\"content\"]\n",
    "    user_text = prompt_messages[-1][\"content\"]\n",
    "    policy = system_text.split(\"Policy:\", 1)[1].strip() if \"Policy:\" in system_text else \"\"\n",
    "    return f\"Ticket:\\n{user_text}\\n\\nPolicy:\\n{policy}\"\n",
    "\n",
    "def score_business(prompts, completions, question: str) -> list[float]:\n",
    "    inputs = [_compose_business_input_from_prompt(p) for p in prompts]\n",
    "    outputs = [c[0][\"content\"] for c in completions]\n",
    "    return [get_pi_score(i, o, question) for i, o in zip(inputs, outputs)]\n",
    "\n",
    "def pi_professional_tone(prompts, completions, **kwargs) -> list[float]:\n",
    "    return score_business(prompts, completions, \"Is the response polite, empathetic, and professional?\")\n",
    "\n",
    "def pi_issue_resolution(prompts, completions, **kwargs) -> list[float]:\n",
    "    return score_business(prompts, completions, \"Does the response directly address the customer's request with clear next steps?\")\n",
    "\n",
    "def pi_policy_adherence(prompts, completions, **kwargs) -> list[float]:\n",
    "    return score_business(prompts, completions, \"Does the response follow the provided policy without contradicting it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3eda9d",
   "metadata": {},
   "source": [
    "## 5) SFT training with TRL’s `SFTTrainer` (LoRA already attached)\n",
    "\n",
    "Because the base is 4-bit **and** LoRA is attached, this avoids the “purely quantized model can’t be fine-tuned” error. We train on the `text` field created above.\n",
    "\n",
    "This is a small, quick run. Scale `num_train_epochs`, batch size, etc., for better quality once you’re happy with the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3766d456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3257bf6be648c48cdd9171278df1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 8 | Num Epochs = 1 | Total steps = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 73,859,072 of 1,617,573,376 (4.57% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT training complete.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"outputs_sft\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=5,\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    fp16=not (torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"SFT training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fdf13",
   "metadata": {},
   "source": [
    "## 6) Save the LoRA adapter\n",
    "\n",
    "You’ll get a small folder that can be reapplied for inference (HF generate or Unsloth’s vLLM fast path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14010f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA to: /home/govind/sft_saved_lora\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "lora_dir = os.path.abspath(\"sft_saved_lora\")\n",
    "try:\n",
    "    model.save_lora(lora_dir)\n",
    "    print(\"Saved LoRA to:\", lora_dir)\n",
    "except Exception as e:\n",
    "    if os.path.isdir(lora_dir):\n",
    "        print(\"LoRA already exists at:\", lora_dir)\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85dea1",
   "metadata": {},
   "source": [
    "## 7) Inference helpers + quick Pi scoring\n",
    "\n",
    "We generate on a few random tickets and score the replies with Pi (tone, resolution, policy). This is just to sanity-check that the pipeline works end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbfdd0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816c8b910e7d4403944b89ce67167c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[0] Ticket: Order #78421 arrived with a cracked mug. Can you replace it? I need it before Friday.\n",
      "Policy: Damaged on arrival: offer free replacement shipped via 2-day; if out of stock, offer full refund.\n",
      "\n",
      "Reply:\n",
      "Dear [Customer],\n",
      "\n",
      "Thank you for reaching out to us. I'm sorry to hear that your order #78421 arrived with a cracked mug. I understand your concern and will do everything possible to replace it for you.\n",
      "\n",
      "I will send you a free replacement mug shipped via 2-day service. If this is not available, I will offer you a full refund.\n",
      "\n",
      "Please let me know if you need any further assistance.\n",
      "\n",
      "Thank you for choosing Contoso Retail.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]  \n",
      "Customer Support\n",
      "\n",
      "Pi Scores — tone: 0.906, resolution: 0.664, policy: 0.977, avg: 0.849\n",
      "================================================================================\n",
      "[2] Ticket: I canceled my order yesterday but I still see a pending charge on my card.\n",
      "Policy: Cancellations void the authorization immediately; banks may take 3–5 business days to release funds.\n",
      "\n",
      "Reply:\n",
      "Dear [Customer's Name],\n",
      "\n",
      "I apologize for any inconvenience this may cause. We appreciate your understanding. Please follow these steps to resolve the issue:\n",
      "\n",
      "1. Check your account balance and transaction history on your bank's website or app.\n",
      "2. Contact your bank's customer service to inquire about the status of the charge and any potential delays in processing.\n",
      "3. If the charge is still pending, you can contact Contoso Retail to request a refund. Please provide your order details and the reason for the cancellation.\n",
      "\n",
      "We will assist you in resolving this issue as soon as possible. Thank you for your patience.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]  \n",
      "Customer Support Team\n",
      "\n",
      "Pi Scores — tone: 0.949, resolution: 0.875, policy: 0.742, avg: 0.855\n",
      "================================================================================\n",
      "[7] Ticket: Do you price match Amazon on the headphones I bought yesterday?\n",
      "Policy: Price match within 14 days against authorized retailers only; Amazon eligible when seller is Amazon.com.\n",
      "\n",
      "Reply:\n",
      "Hi there! I'm sorry to hear that you're experiencing this issue. I'm here to help you. Could you please provide your order number and the product details so I can look into it? Thank you for your understanding.\n",
      "\n",
      "Pi Scores — tone: 0.894, resolution: 0.498, policy: 0.445, avg: 0.613\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "def _chat_to_text(msgs: List[Dict[str, str]]) -> str:\n",
    "    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def generate_batch(prompts: List[List[Dict[str,str]]], max_tokens: int = 180) -> List[str]:\n",
    "    try:\n",
    "        from vllm import SamplingParams\n",
    "        params = SamplingParams(temperature=0.4, top_p=0.9, max_tokens=max_tokens)\n",
    "        texts = [_chat_to_text(p) for p in prompts]\n",
    "        outs = model.fast_generate(texts, sampling_params=params, lora_request=None)\n",
    "        if outs and hasattr(outs[0], \"outputs\"):\n",
    "            return [o.outputs[0].text for o in outs]\n",
    "        if outs and hasattr(outs[0], \"text\"):\n",
    "            return [o.text for o in outs]\n",
    "        return [str(o) for o in outs]\n",
    "    except Exception:\n",
    "        # HF generate fallback\n",
    "        results = []\n",
    "        for p in prompts:\n",
    "            txt = _chat_to_text(p)\n",
    "            toks = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **toks,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.4,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            gen = out[0][toks[\"input_ids\"].shape[1]:]\n",
    "            results.append(tokenizer.decode(gen, skip_special_tokens=True).strip())\n",
    "        return results\n",
    "\n",
    "def sample_and_score(n=3, seed=123):\n",
    "    random.seed(seed)\n",
    "    idxs = random.sample(range(len(dataset)), k=min(n, len(dataset)))\n",
    "    prompts = [dataset[i][\"prompt\"] for i in idxs]\n",
    "    replies = generate_batch(prompts)\n",
    "    tone = score_business(prompts, [[{\"role\": \"assistant\", \"content\": r}] for r in replies], \"Is the response polite, empathetic, and professional?\")\n",
    "    reso = score_business(prompts, [[{\"role\": \"assistant\", \"content\": r}] for r in replies], \"Does the response directly address the customer's request with clear next steps?\")\n",
    "    polc = score_business(prompts, [[{\"role\": \"assistant\", \"content\": r}] for r in replies], \"Does the response follow the provided policy without contradicting it?\")\n",
    "    for j, i in enumerate(idxs):\n",
    "        sys = prompts[j][0][\"content\"]\n",
    "        ticket = prompts[j][1][\"content\"]\n",
    "        policy = sys.split(\"Policy:\", 1)[1].strip() if \"Policy:\" in sys else \"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"[{i}] Ticket: {ticket}\\nPolicy: {policy}\\n\\nReply:\\n{replies[j].strip()}\")\n",
    "        avg = (tone[j] + reso[j] + polc[j]) / 3.0\n",
    "        print(f\"\\nPi Scores — tone: {tone[j]:.3f}, resolution: {reso[j]:.3f}, policy: {polc[j]:.3f}, avg: {avg:.3f}\")\n",
    "\n",
    "sample_and_score(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfac1c4",
   "metadata": {},
   "source": [
    "## 8) (Optional) Azure AI Foundry — upload dataset and run an evaluation\n",
    "\n",
    "This cell generates a small JSONL with `{query, response, policy, input, output}`, uploads it to your Azure AI Project, and launches an evaluation with the built-in **Relevance** evaluator. If you’ve registered a custom evaluator, set `CUSTOM_EVALUATOR_ID` and it will be included too.\n",
    "\n",
    "Auth options:\n",
    "- Preferred: set `AZURE_AI_PROJECT` and rely on `DefaultAzureCredential()` (e.g., `az login` or Managed Identity on your compute).\n",
    "- Legacy preview: `AZURE_AI_PROJECT_CONNECTION_STRING` also works if your SDK build still supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcefd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pathlib\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import Evaluation, InputDataset, DatasetInputType, EvaluatorConfiguration, EvaluatorIds\n",
    "\n",
    "def _gen_reply(messages):\n",
    "    # Use HF generate for portability across environments\n",
    "    txt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    toks = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**toks, max_new_tokens=200, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "    gen = out[0][toks[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "sample = dataset.select(range(min(32, len(dataset))))\n",
    "rows = []\n",
    "for ex in sample:\n",
    "    reply = _gen_reply(ex[\"prompt\"])\n",
    "    system_text = ex[\"prompt\"][0][\"content\"]\n",
    "    user_text = ex[\"prompt\"][-1][\"content\"]\n",
    "    policy = system_text.split(\"Policy:\", 1)[1].strip() if \"Policy:\" in system_text else \"\"\n",
    "    rows.append({\n",
    "        \"query\": user_text,\n",
    "        \"response\": reply,\n",
    "        \"policy\": policy,\n",
    "        \"input\": f\"Ticket:\\n{user_text}\\n\\nPolicy:\\n{policy}\",\n",
    "        \"output\": reply\n",
    "    })\n",
    "\n",
    "jsonl_path = pathlib.Path(\"azure_eval_dataset.jsonl\")\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Wrote {len(rows)} rows -> {jsonl_path.resolve()}\")\n",
    "\n",
    "conn_str = os.environ.get(\"AZURE_AI_PROJECT_CONNECTION_STRING\")\n",
    "if conn_str:\n",
    "    project_client = AIProjectClient.from_connection_string(conn_str)\n",
    "else:\n",
    "    endpoint = os.environ.get(\"AZURE_AI_PROJECT\")\n",
    "    if not endpoint:\n",
    "        raise RuntimeError(\"Set AZURE_AI_PROJECT or AZURE_AI_PROJECT_CONNECTION_STRING to run this cell.\")\n",
    "    project_client = AIProjectClient(endpoint=endpoint, credential=DefaultAzureCredential())\n",
    "\n",
    "with project_client:\n",
    "    dataset_id = project_client.datasets.upload_file(path=str(jsonl_path))\n",
    "    print(\"Uploaded dataset id:\", dataset_id)\n",
    "\n",
    "    evaluators = [\n",
    "        EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.RELEVANCE.value,\n",
    "            data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "            settings={\"definition\": \"Is the agent’s reply relevant and does it address the customer’s request?\"}\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    custom_eval_id = os.environ.get(\"CUSTOM_EVALUATOR_ID\")\n",
    "    if custom_eval_id:\n",
    "        evaluators.append(\n",
    "            EvaluatorConfiguration(\n",
    "                id=custom_eval_id,\n",
    "                data_mapping={\"input\": \"${data.input}\", \"output\": \"${data.output}\", \"policy\": \"${data.policy}\"}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    headers = {}\n",
    "    if all(k in os.environ for k in (\"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_DEPLOYMENT\")):\n",
    "        headers = {\n",
    "            \"model-endpoint\": os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            \"api-key\": os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            \"azureml-model-deployment\": os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    "        }\n",
    "\n",
    "    eval_job = project_client.evaluations.create(\n",
    "        evaluation=Evaluation(\n",
    "            name=\"contoso-cs-email-eval\",\n",
    "            data=InputDataset(type=DatasetInputType.URI_FILE_DATASET, path=dataset_id),\n",
    "            evaluators=evaluators\n",
    "        ),\n",
    "        headers=headers\n",
    "    )\n",
    "    print(\"Submitted evaluation job id:\", eval_job)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
