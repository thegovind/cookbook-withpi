{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a01fe7",
   "metadata": {},
   "source": [
    "# GRPO fine-tuning with Pi‑Scorer rewards on **Phi‑4 Mini Instruct** (LoRA/PEFT)\n",
    "\n",
    "This end‑to‑end notebook shows how to:\n",
    "\n",
    "1) Load **Phi‑4 Mini Instruct** quantized to 4‑bit and enable **LoRA/PEFT** with [Unsloth](https://unsloth.ai/).\n",
    "2) Train with **GRPO** (Group Relative Policy Optimization) using **Pi‑Scorer** functions as rewards.\n",
    "3) Save and reuse the LoRA adapter.\n",
    "4) Evaluate locally and in **Azure AI Foundry** (Projects SDK) using built‑in and custom evaluators.\n",
    "\n",
    "> Why these choices?\n",
    "- **Unsloth** provides an optimized GRPO trainer and tight integration with PEFT/LoRA, vLLM fast generation, and quantized loading.\n",
    "- **GRPO** stabilizes RL for LLMs by comparing rollouts within a group.\n",
    "- **Pi‑Scorer** lets you replace expensive LLM‑as‑a‑judge with fast, deterministic scorers.\n",
    "\n",
    "References:\n",
    "- Unsloth GRPO tutorial (methods/config & saving LoRA).  \n",
    "  - See: *GRPO & vLLM* and *save_lora()* usage in Unsloth docs.  \n",
    "- Azure AI Foundry Evaluations (built‑in evaluators, Projects SDK).  \n",
    "  - See the Evaluate how‑to & Projects SDK API for Evaluations in Azure docs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e032f",
   "metadata": {},
   "source": [
    "## 0) Installs\n",
    "\n",
    "We install Unsloth, TRL (for the GRPOConfig/GRPOTrainer API), vLLM (fast gen path), and support libs.\n",
    "\n",
    "**Tip:** If you're running this on a fresh environment, the first cell can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pinned/compatible packages for this workflow.\n",
    "!pip -q install unsloth vllm==0.8.5.post1\n",
    "!pip -q install bitsandbytes accelerate xformers peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
    "!pip -q install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "!pip -q install transformers==4.51.3 ipywidgets requests\n",
    "!pip -q install azure-ai-projects azure-identity  # Optional: Azure AI Foundry Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe684b",
   "metadata": {},
   "source": [
    "## 1) Configure API keys (Pi‑Scorer & optional Azure)\n",
    "\n",
    "We read **WITHPI_API_KEY** for Pi‑Scorer (from https://build.withpi.ai) and (optionally) Azure variables for evaluations.\n",
    "\n",
    "Nothing is sent anywhere in this cell — it just checks environment variables and prompts if missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _need(key: str, prompt: str) -> str:\n",
    "    val = os.environ.get(key)\n",
    "    if val:\n",
    "        return val\n",
    "    try:\n",
    "        val = getpass.getpass(prompt)\n",
    "    except Exception:\n",
    "        val = input(prompt)\n",
    "    if val:\n",
    "        os.environ[key] = val.strip()\n",
    "    return os.environ.get(key, \"\")\n",
    "\n",
    "# Required for Pi-Scorer reward calls\n",
    "_need(\"WITHPI_API_KEY\", \"Enter your WITHPI_API_KEY (input hidden): \")\n",
    "\n",
    "# Optional: If you plan to use Azure AI Foundry Evaluations via Projects SDK\n",
    "# os.environ.setdefault(\"AZURE_AI_PROJECT_CONNECTION_STRING\", \"Endpoint=...;Project=...;SubscriptionId=...;ResourceGroup=...\")\n",
    "# os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\", \"https://<your-aoai>.openai.azure.com/\")\n",
    "# os.environ.setdefault(\"AZURE_OPENAI_API_KEY\", \"<key>\")\n",
    "# os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\", \"<gpt-4o-mini / o4-mini / etc>\")\n",
    "\n",
    "print(\"WITHPI_API_KEY set?\", bool(os.environ.get(\"WITHPI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c190c1",
   "metadata": {},
   "source": [
    "## 2) Load **Phi‑4 Mini Instruct** (4‑bit) and enable **LoRA/PEFT**\n",
    "\n",
    "We use Unsloth's `FastLanguageModel` loader to pull a 4‑bit model and then attach LoRA adapters. PEFT training drastically reduces GPU memory use compared to full‑precision full‑parameter RL fine‑tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "# Cache locations (helps on repeated runs)\n",
    "os.environ.setdefault(\"HF_HOME\", \"/mnt/hf/cache\")\n",
    "os.environ.setdefault(\"HF_HUB_CACHE\", \"/mnt/hf/cache/hub\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", \"/mnt/hf/cache/hub\")\n",
    "\n",
    "max_seq_length = 1024\n",
    "lora_rank = 64\n",
    "model_name = \"unsloth/Phi-4-mini-instruct-bnb-4bit\"  # 4-bit, fast to load\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5,\n",
    "    trust_remote_code = True,\n",
    "    cache_dir = os.environ.get(\"HF_HUB_CACHE\", None),\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loaded:\", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47ca95",
   "metadata": {},
   "source": [
    "## 3) Create a **business support** scenario dataset\n",
    "\n",
    "We'll train the assistant to write short, policy‑compliant customer‑support emails. Each item has a *ticket* and a *policy*. The **system** message includes the policy; the **user** message contains the ticket. GRPO will sample responses and Pi‑Scorer will grade them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b5bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a customer support agent for Contoso Retail. Write a short, professional email reply that:\n",
    "- acknowledges the customer’s issue empathetically,\n",
    "- provides clear next steps or resolution,\n",
    "- follows the policy provided (do not contradict it),\n",
    "- stays concise (≤150 words).\n",
    "\"\"\"\n",
    "\n",
    "business_samples = [\n",
    "    {\n",
    "        \"ticket\": \"Order #78421 arrived with a cracked mug. Can you replace it? I need it before Friday.\",\n",
    "        \"policy\": \"Damaged on arrival: offer free replacement shipped via 2-day; if out of stock, offer full refund.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"I’m past the 30-day window but this shirt still has tags. Any chance I can return it?\",\n",
    "        \"policy\": \"Returns accepted within 30 days only; exceptions allowed as store credit at manager discretion.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"I canceled my order yesterday but I still see a pending charge on my card.\",\n",
    "        \"policy\": \"Cancellations void the authorization immediately; banks may take 3–5 business days to release funds.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"The promo code SPRING25 didn’t apply at checkout. Can you refund the difference?\",\n",
    "        \"policy\": \"SPRING25: 25% off full-price items only; cannot be combined; adjustments allowed within 7 days of purchase.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"I need to change the shipping address on my order to my office downtown.\",\n",
    "        \"policy\": \"Address changes allowed until fulfillment starts; otherwise reroute via carrier once tracking is issued.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"My gift card shows $0 after one use but I only spent $12 of $25.\",\n",
    "        \"policy\": \"Gift cards decrement in real-time; if balance mismatch occurs, reissue a new card with remaining funds.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"The espresso machine stopped working after two weeks. What can I do?\",\n",
    "        \"policy\": \"Appliances: 1-year warranty. Offer troubleshooting; if unresolved, advance replacement or repair.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"Do you price match Amazon on the headphones I bought yesterday?\",\n",
    "        \"policy\": \"Price match within 14 days against authorized retailers only; Amazon eligible when seller is Amazon.com.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(business_samples)\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT + \"\\n\\nPolicy:\\n\" + x[\"policy\"]},\n",
    "            {\"role\": \"user\", \"content\": x[\"ticket\"]},\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce25c3",
   "metadata": {},
   "source": [
    "## 4) Define **Pi‑Scorer** reward functions\n",
    "\n",
    "Each reward returns a scalar in \\[0,1] for a (prompt, completion) pair. We'll call the Pi API for three questions: **professional tone**, **issue resolution**, and **policy adherence**.\n",
    "\n",
    "In GRPO, the *relative* ranking of samples in a group matters most — perfect calibration isn't required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b74130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "\n",
    "PI_API_KEY = os.environ.get(\"WITHPI_API_KEY\")\n",
    "PI_API_URL = \"https://api.withpi.ai/v1/scoring_system/score\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"x-api-key\": PI_API_KEY}\n",
    "\n",
    "def get_pi_score(input_text: str, output_text: str, question: str) -> float:\n",
    "    \"\"\"Call Pi‑Scorer for a single (input, output, question). Returns float score.\"\"\"\n",
    "    payload = {\n",
    "        \"llm_input\": input_text,\n",
    "        \"llm_output\": output_text,\n",
    "        \"scoring_spec\": [{\"question\": question}],\n",
    "    }\n",
    "    resp = requests.post(PI_API_URL, headers=HEADERS, json=payload, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if \"total_score\" not in data:\n",
    "        raise KeyError(\"'total_score' missing in Pi response\")\n",
    "    return float(data[\"total_score\"])\n",
    "\n",
    "def _compose_business_input_from_prompt(prompt_messages: list[dict]) -> str:\n",
    "    # Extract the ticket & policy we inserted in the system/user turns\n",
    "    system_text = prompt_messages[0][\"content\"]\n",
    "    user_text = prompt_messages[-1][\"content\"]\n",
    "    policy = system_text.split(\"Policy:\", 1)[1].strip() if \"Policy:\" in system_text else \"\"\n",
    "    return f\"Ticket:\\n{user_text}\\n\\nPolicy:\\n{policy}\"\n",
    "\n",
    "def score_business(prompts, completions, question: str) -> list[float]:\n",
    "    # prompts: list[list[{role, content}]]\n",
    "    # completions: list[list[{role: 'assistant', content: str}]]\n",
    "    inputs = [_compose_business_input_from_prompt(p) for p in prompts]\n",
    "    outputs = [c[0][\"content\"] for c in completions]\n",
    "    return [get_pi_score(i, o, question) for i, o in zip(inputs, outputs)]\n",
    "\n",
    "def pi_professional_tone(prompts, completions, **kwargs) -> list[float]:\n",
    "    return score_business(prompts, completions, \"Is the response polite, empathetic, and professional?\")\n",
    "\n",
    "def pi_issue_resolution(prompts, completions, **kwargs) -> list[float]:\n",
    "    return score_business(prompts, completions, \"Does the response directly address the customer's request with clear next steps?\")\n",
    "\n",
    "def pi_policy_adherence(prompts, completions, **kwargs) -> list[float]:\n",
    "    return score_business(prompts, completions, \"Does the response follow the provided policy without contradicting it?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27462b1b",
   "metadata": {},
   "source": [
    "## 5) GRPO config **and** a minimal wrapper to satisfy Unsloth's trainer\n",
    "\n",
    "Unsloth's GRPO trainer sometimes expects policy models to expose extra helpers. Two sticking points we solve here:\n",
    "\n",
    "- The trainer writes a flag to `model.warnings_issued[...]` — we provide a dict.\n",
    "- The Unsloth trainer calls `model.add_model_tags(tags)` during init — our wrapper forwards that (or no‑ops).\n",
    "\n",
    "We also return **last hidden states** via `.logits` to match trainer expectations without altering the base model. This avoids attribute errors like `HiddenAsLogitsWrapper has no attribute 'add_model_tags'` and earlier recursion issues when accessing `base_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ba90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "import torch, torch.nn as nn\n",
    "import types\n",
    "\n",
    "# Keep GRPO on HF path (disable in-trainer vLLM); we still use vLLM for inference.\n",
    "def _bf16_supported() -> bool:\n",
    "    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = False,                 # critical: avoid vLLM engine inside trainer\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = _bf16_supported(),\n",
    "    fp16 = not _bf16_supported(),\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    num_generations = 8,\n",
    "    max_prompt_length = 1024,\n",
    "    max_completion_length = 200,\n",
    "    max_steps = 40,                   # toy run; increase for real training\n",
    "    save_steps = 10,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"outputs\",\n",
    ")\n",
    "\n",
    "class HiddenAsLogitsWrapper(nn.Module):\n",
    "    \"\"\"Wrap a (PEFT) CausalLM so forward(...).logits returns LAST HIDDEN STATES (B,S,H).\n",
    "    Also: expose trainer-expected attrs/methods and delegate unknowns to the base model.\n",
    "    This fixes AttributeErrors like `add_model_tags` and avoids recursion when accessing `base_model`.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        # IMPORTANT: assign base_model before any code that might touch __getattr__\n",
    "        object.__setattr__(self, \"base_model\", base_model)\n",
    "        # Commonly-inspected attributes\n",
    "        self.config = getattr(base_model, \"config\", None)\n",
    "        self.lm_head = getattr(base_model, \"lm_head\", None)\n",
    "        # Trainer writes to this; make sure it's present & mutable\n",
    "        self.warnings_issued = getattr(base_model, \"warnings_issued\", {}) or {}\n",
    "\n",
    "    # ============ Core forward: return last hidden states as `.logits` ============\n",
    "    def forward(self, *args, **kwargs):\n",
    "        kwargs = dict(kwargs)\n",
    "        kwargs[\"output_hidden_states\"] = True\n",
    "        kwargs[\"return_dict\"] = True\n",
    "        out = self.base_model(*args, **kwargs)\n",
    "        last_hidden = out.hidden_states[-1]  # (B, S, H)\n",
    "        return types.SimpleNamespace(logits=last_hidden)\n",
    "\n",
    "    # ============ Trainer convenience hooks ============\n",
    "    def add_model_tags(self, tags):\n",
    "        # Forward if implemented by the base model, else record & no-op\n",
    "        if hasattr(self.base_model, \"add_model_tags\"):\n",
    "            return self.base_model.add_model_tags(tags)\n",
    "        self._wrapped_model_tags = list(tags) if isinstance(tags, (list, tuple, set)) else [tags]\n",
    "        return None\n",
    "\n",
    "    # ============ Delegate helpers to preserve normal behavior ============\n",
    "    def get_output_embeddings(self):\n",
    "        return self.base_model.get_output_embeddings()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.base_model.get_input_embeddings()\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.base_model.generate(*args, **kwargs)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        self.base_model.train(mode)\n",
    "        return super().train(mode)\n",
    "\n",
    "    def eval(self):\n",
    "        self.base_model.eval()\n",
    "        return super().eval()\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self.base_model.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        try:\n",
    "            return next(self.base_model.parameters()).device\n",
    "        except StopIteration:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    # ============ Robust attribute forwarding without recursion ============\n",
    "    def __getattr__(self, name):\n",
    "        # If base_model isn't set yet, fall back to default to avoid recursion\n",
    "        if name == \"base_model\":\n",
    "            return object.__getattribute__(self, \"base_model\")\n",
    "        try:\n",
    "            return super().__getattribute__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name)\n",
    "\n",
    "grpo_model = HiddenAsLogitsWrapper(model)\n",
    "print(\"Wrapper ready. add_model_tags?\", hasattr(grpo_model, \"add_model_tags\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27cbc13",
   "metadata": {},
   "source": [
    "## 6) Train with GRPO\n",
    "\n",
    "We point the Unsloth GRPO trainer at our wrapped model, dataset, tokenizer (as `processing_class`), and the three Pi reward functions.\n",
    "\n",
    "> **Note:** This is a short run (`max_steps=40`, `num_generations=8`) so it fits modest GPUs. Increase for real training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Patch: return LAST HIDDEN STATES in .logits so Unsloth GRPO can matmul with lm_head ---\n",
    "import types\n",
    "import torch\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "def patch_logits_to_hidden_states(model):\n",
    "    if not hasattr(model, \"warnings_issued\"):\n",
    "        model.warnings_issued = {}\n",
    "    # keep original forward\n",
    "    _orig_forward = model.forward\n",
    "\n",
    "    def _forward(*args, **kwargs):\n",
    "        kwargs = dict(kwargs)\n",
    "        # force hidden states + dict outputs\n",
    "        kwargs[\"output_hidden_states\"] = True\n",
    "        kwargs[\"return_dict\"] = True\n",
    "        out = _orig_forward(*args, **kwargs)\n",
    "\n",
    "        # pull last hidden states robustly\n",
    "        hidden = None\n",
    "        if hasattr(out, \"hidden_states\") and out.hidden_states is not None:\n",
    "            hidden = out.hidden_states[-1]              # (B, S, H)\n",
    "        elif isinstance(out, (tuple, list)) and len(out) >= 3 and out[2] is not None:\n",
    "            hidden = out[2][-1]                         # (B, S, H) for tuple returns\n",
    "        if hidden is None:\n",
    "            raise RuntimeError(\"Hidden states not returned by model; cannot train GRPO.\")\n",
    "\n",
    "        # If the output object lets us overwrite .logits, do it in place.\n",
    "        try:\n",
    "            out.logits = hidden                         # <- make logits be (B, S, H)\n",
    "            return out\n",
    "        except Exception:\n",
    "            # Fall back to constructing a fresh, standard output\n",
    "            return CausalLMOutputWithPast(\n",
    "                logits=hidden,\n",
    "                past_key_values=getattr(out, \"past_key_values\", None),\n",
    "                hidden_states=getattr(out, \"hidden_states\", None),\n",
    "                attentions=getattr(out, \"attentions\", None),\n",
    "            )\n",
    "\n",
    "    # monkey-patch\n",
    "    model.forward = types.MethodType(_forward, model)\n",
    "    return model\n",
    "\n",
    "# apply the patch\n",
    "model = patch_logits_to_hidden_states(model)\n",
    "\n",
    "# (Optional) quick sanity check on shapes\n",
    "with torch.no_grad():\n",
    "    tok = tokenizer(\"hi\", return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    dbg = model(**tok)\n",
    "    print(\"Patched logits shape (should be B,S,H):\", tuple(dbg.logits.shape))\n",
    "    emb = model.get_output_embeddings().weight\n",
    "    print(\"Output embedding (V,H):\", tuple(emb.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm=False,                 # <-- important: disable vLLM during GRPO\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=4,  # multiple of num_generations\n",
    "    num_generations=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_prompt_length=1024,\n",
    "    max_completion_length=200,\n",
    "    max_steps=20,                   # toy\n",
    "    save_steps=10,\n",
    "    bf16=True,                      # H100: yes\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"outputs\",\n",
    ")\n",
    "\n",
    "# left pad for GRPO sampling\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,                    # <- the PEFT model, **after** patch_logits_to_hidden_states()\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[pi_professional_tone, pi_issue_resolution, pi_policy_adherence],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758f4ea",
   "metadata": {},
   "source": [
    "## 7) Save the LoRA adapter\n",
    "\n",
    "This gives you a small adapter folder you can mount later for inference (HF generate **or** vLLM fast path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "lora_dir = os.path.abspath(\"grpo_saved_lora\")\n",
    "try:\n",
    "    model.save_lora(lora_dir)  # Unsloth helper\n",
    "    print(\"Saved LoRA to:\", lora_dir)\n",
    "except Exception as e:\n",
    "    if os.path.isdir(lora_dir):\n",
    "        print(\"LoRA already exists at:\", lora_dir)\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42603d2f",
   "metadata": {},
   "source": [
    "## 8) Quick local inference helpers (vLLM fast path when available)\n",
    "\n",
    "Below we: (a) warm up; (b) define a batch inference helper; (c) score a few random examples with Pi‑Scorer to see progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f59189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"bfloat16 supported:\", torch.cuda.is_bf16_supported())\n",
    "    try:\n",
    "        total, free = torch.cuda.get_device_properties(0).total_memory, torch.cuda.mem_get_info()[0]\n",
    "        print(\"VRAM (total / free GB):\", round(total/1e9,2), \"/\", round(free/1e9,2))\n",
    "    except Exception as e:\n",
    "        print(\"VRAM info unavailable:\", e)\n",
    "\n",
    "def _warm_up(model, tokenizer):\n",
    "    \"\"\"One tiny generation to compile kernels/caches.\"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Say hello in 5 words.\"},\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        from vllm import SamplingParams\n",
    "        out = model.fast_generate([prompt], sampling_params=SamplingParams(max_tokens=16), lora_request=None)\n",
    "        elapsed = time.time() - t0\n",
    "        print(\"Warm-up (vLLM)\", f\"{elapsed:.2f}s\")\n",
    "    except Exception as e:\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=16, pad_token_id=tokenizer.eos_token_id)\n",
    "        print(\"Warm-up (HF generate)\")\n",
    "\n",
    "_warm_up(model, tokenizer)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "def _chat_to_prompt_text(messages: List[Dict[str, str]]) -> str:\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def _normalize_outputs(outs: List[Any]) -> List[str]:\n",
    "    if not outs:\n",
    "        return []\n",
    "    if isinstance(outs[0], str):\n",
    "        return outs\n",
    "    if hasattr(outs[0], \"outputs\"):\n",
    "        return [(o.outputs[0].text if getattr(o, \"outputs\", None) else \"\") for o in outs]\n",
    "    if hasattr(outs[0], \"text\"):\n",
    "        return [getattr(o, \"text\", str(o)) for o in outs]\n",
    "    return [str(o) for o in outs]\n",
    "\n",
    "def _infer_batch(prompts: List[List[Dict[str, str]]],\n",
    "                 temperature: float = 0.4,\n",
    "                 top_p: float = 0.9,\n",
    "                 max_tokens: int = 180,\n",
    "                 lora_request=None) -> Tuple[List[str], dict]:\n",
    "    texts = [_chat_to_prompt_text(p) for p in prompts]\n",
    "    params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
    "    raw_outs = model.fast_generate(texts, sampling_params=params, lora_request=lora_request)\n",
    "    replies = _normalize_outputs(raw_outs)\n",
    "    return replies, {\"backend\": \"vllm\", \"num\": len(replies)}\n",
    "\n",
    "def run_business_eval(n: int = 3, score_with_pi: bool = True, seed: int = 123, **gen_kwargs) -> list:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    n = min(n, len(dataset))\n",
    "    idxs = random.sample(range(len(dataset)), k=n)\n",
    "    prompts = [dataset[i][\"prompt\"] for i in idxs]\n",
    "    outs, stats = _infer_batch(prompts, **gen_kwargs)\n",
    "    rows = []\n",
    "    if score_with_pi:\n",
    "        tone_scores = score_business(prompts, [[{\"role\": \"assistant\", \"content\": o}] for o in outs],\n",
    "                                     \"Is the response polite, empathetic, and professional?\")\n",
    "        res_scores  = score_business(prompts, [[{\"role\": \"assistant\", \"content\": o}] for o in outs],\n",
    "                                     \"Does the response directly address the customer's request with clear next steps?\")\n",
    "        policy_scores = score_business(prompts, [[{\"role\": \"assistant\", \"content\": o}] for o in outs],\n",
    "                                     \"Does the response follow the provided policy without contradicting it?\")\n",
    "    for j, i in enumerate(idxs):\n",
    "        p = prompts[j]\n",
    "        sys = p[0][\"content\"]\n",
    "        ticket = p[1][\"content\"]\n",
    "        policy = sys.split(\"Policy:\", 1)[1].strip() if \"Policy:\" in sys else \"\"\n",
    "        row = {\"index\": int(i), \"ticket\": ticket, \"policy\": policy, \"reply\": outs[j].strip()}\n",
    "        if score_with_pi:\n",
    "            row.update({\n",
    "                \"pi_tone\": float(tone_scores[j]),\n",
    "                \"pi_resolution\": float(res_scores[j]),\n",
    "                \"pi_policy\": float(policy_scores[j]),\n",
    "                \"pi_avg\": float((tone_scores[j] + res_scores[j] + policy_scores[j]) / 3.0),\n",
    "            })\n",
    "        rows.append(row)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"[{row['index']}] Ticket: {row['ticket']}\\nPolicy: {row['policy']}\\n\\nReply:\\n{row['reply']}\")\n",
    "        if score_with_pi:\n",
    "            print(f\"\\nPi Scores — tone: {row['pi_tone']:.3f}, resolution: {row['pi_resolution']:.3f}, policy: {row['pi_policy']:.3f}, avg: {row['pi_avg']:.3f}\")\n",
    "    return rows\n",
    "\n",
    "_ = run_business_eval(n=3, score_with_pi=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9558907",
   "metadata": {},
   "source": [
    "## 9) (Optional) Azure AI Foundry: run a cloud evaluation\n",
    "\n",
    "We generate a small JSONL from the dataset + model replies, upload it to your Azure AI Project, and launch an evaluation using built‑in **Relevance** (and optionally a custom evaluator you registered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c37e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pathlib\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import Evaluation, InputDataset, DatasetInputType, EvaluatorConfiguration, EvaluatorIds\n",
    "\n",
    "def _gen_reply(messages):\n",
    "    # Simple HF-generate for portability (no vLLM requirement on the compute that runs this cell)\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    toks = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**toks, max_new_tokens=200, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "    gen = out[0][toks[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "sample = dataset.select(range(min(32, len(dataset))))\n",
    "rows = []\n",
    "for ex in sample:\n",
    "    reply = _gen_reply(ex[\"prompt\"])\n",
    "    system_text = ex[\"prompt\"][0][\"content\"]\n",
    "    user_text = ex[\"prompt\"][-1][\"content\"]\n",
    "    policy = system_text.split(\"Policy:\", 1)[1].strip() if \"Policy:\" in system_text else \"\"\n",
    "    rows.append({\n",
    "        \"query\": user_text,\n",
    "        \"response\": reply,\n",
    "        \"policy\": policy,\n",
    "        \"input\": f\"Ticket:\\n{user_text}\\n\\nPolicy:\\n{policy}\",\n",
    "        \"output\": reply,\n",
    "    })\n",
    "\n",
    "jsonl_path = pathlib.Path(\"azure_eval_dataset.jsonl\")\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Wrote {len(rows)} rows -> {jsonl_path.resolve()}\")\n",
    "\n",
    "conn_str = os.environ.get(\"AZURE_AI_PROJECT_CONNECTION_STRING\")\n",
    "if not conn_str and not os.environ.get(\"AZURE_AI_PROJECT\"):\n",
    "    print(\"Skip: set AZURE_AI_PROJECT_CONNECTION_STRING or AZURE_AI_PROJECT to run this cell.\")\n",
    "else:\n",
    "    if conn_str:\n",
    "        project_client = AIProjectClient.from_connection_string(conn_str)\n",
    "    else:\n",
    "        project_client = AIProjectClient(endpoint=os.environ[\"AZURE_AI_PROJECT\"], credential=DefaultAzureCredential())\n",
    "\n",
    "    with project_client:\n",
    "        dataset_id = project_client.datasets.upload_file(path=str(jsonl_path))\n",
    "        print(\"Uploaded dataset id:\", dataset_id)\n",
    "\n",
    "        evaluators = [\n",
    "            EvaluatorConfiguration(\n",
    "                id=EvaluatorIds.RELEVANCE.value,\n",
    "                data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "                settings={\"definition\": \"Is the agent’s reply relevant and does it address the customer’s request?\"}\n",
    "            )\n",
    "        ]\n",
    "        custom_eval_id = os.environ.get(\"CUSTOM_EVALUATOR_ID\")\n",
    "        if custom_eval_id:\n",
    "            evaluators.append(\n",
    "                EvaluatorConfiguration(\n",
    "                    id=custom_eval_id,\n",
    "                    data_mapping={\"input\": \"${data.input}\", \"output\": \"${data.output}\", \"policy\": \"${data.policy}\"},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        headers = {}\n",
    "        if all(k in os.environ for k in (\"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_DEPLOYMENT\")):\n",
    "            headers = {\n",
    "                \"model-endpoint\": os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "                \"api-key\": os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "                \"azureml-model-deployment\": os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "            }\n",
    "\n",
    "        eval_job = project_client.evaluations.create(\n",
    "            evaluation=Evaluation(\n",
    "                name=\"contoso-cs-email-eval\",\n",
    "                data=InputDataset(type=DatasetInputType.URI_FILE_DATASET, path=dataset_id),\n",
    "                evaluators=evaluators,\n",
    "            ),\n",
    "            headers=headers,\n",
    "        )\n",
    "        print(\"Submitted evaluation job id:\", eval_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c37b57",
   "metadata": {},
   "source": [
    "---\n",
    "**Notes**\n",
    "- If you want to serve the base model + LoRA with vLLM elsewhere, construct a `LoRARequest` (signature varies across vLLM versions) and pass it to `model.fast_generate(..., lora_request=...)`.\n",
    "- For a larger training, scale `num_generations`, `max_steps`, and batch size gradually; monitor GPU VRAM.\n",
    "- Always verify your policies in the system message and ensure your reward questions match business goals.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
